{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7ghYEg0J14k",
        "outputId": "b00815fe-db8d-4aee-d5f1-7cde08cc60be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing requirements.txt\n"
          ]
        }
      ],
      "source": [
        "%%writefile requirements.txt\n",
        "torch==2.7.1\n",
        "transformers==4.44.2\n",
        "scikit-learn==1.6.1\n",
        "pandas==2.3.0\n",
        "numpy==2.2.6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "t1C1XpB1Oqf4"
      },
      "outputs": [],
      "source": [
        "# !pip install -r requirements.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LWYJEJpaL6Mo"
      },
      "outputs": [],
      "source": [
        "!mkdir app inputs outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTQWxoMDLxwf",
        "outputId": "22e06ab3-0e7d-499d-f638-e451a64d2cab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing app/bot.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app/bot.py\n",
        "import pickle\n",
        "import socket\n",
        "import atexit\n",
        "\n",
        "# Known bot suffixes used for reverse DNS validation\n",
        "KNOWN_BOTS = {\n",
        "    \"Googlebot\": [\".googlebot.com\"],\n",
        "    \"Bingbot\": [\".search.msn.com\"],\n",
        "    \"AhrefsBot\": [\".ahrefs.com\", \".ahrefs.net\"],\n",
        "    \"YandexBot\": [\".yandex.ru\", \".yandex.com\", \".yandex.net\"],\n",
        "    \"SemrushBot\": [\".semrush.com\"],\n",
        "    \"DuckDuckBot\": [\".duckduckgo.com\"],\n",
        "    \"MJ12bot\": [\".majestic12.co.uk\"],\n",
        "    \"Slurp\": [\".crawl.yahoo.net\"],\n",
        "    \"Applebot\": [\".apple.com\"]\n",
        "}\n",
        "\n",
        "# Cache file paths for verified and spoofed bot IPs\n",
        "VERIFIED_IP_FILE = \"verified_bots.pkl\"\n",
        "SPOOFED_IP_FILE = \"spoofed_bots.pkl\"\n",
        "\n",
        "def pickle_load(path):\n",
        "    \"\"\"\n",
        "    Load a Python set object from a pickle file.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(path, \"rb\") as f:\n",
        "            return pickle.load(f)\n",
        "    except Exception:\n",
        "        return set()\n",
        "\n",
        "# Load IP caches early to avoid NameError during runtime\n",
        "verified_bot_ips = pickle_load(VERIFIED_IP_FILE)\n",
        "spoofed_bot_ips = pickle_load(SPOOFED_IP_FILE)\n",
        "\n",
        "def save_ip_caches():\n",
        "    \"\"\"\n",
        "    Persist the sets of verified and spoofed bot IP addresses to disk.\n",
        "    \"\"\"\n",
        "    with open(VERIFIED_IP_FILE, \"wb\") as f:\n",
        "        pickle.dump(verified_bot_ips, f)\n",
        "    with open(SPOOFED_IP_FILE, \"wb\") as f:\n",
        "        pickle.dump(spoofed_bot_ips, f)\n",
        "\n",
        "atexit.register(save_ip_caches)\n",
        "\n",
        "def reverse_dns(ip):\n",
        "    \"\"\"\n",
        "    Perform a reverse DNS lookup for a given IP address.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        return socket.gethostbyaddr(ip)[0]\n",
        "    except socket.herror:\n",
        "        return None\n",
        "\n",
        "def forward_dns(hostname):\n",
        "    \"\"\"\n",
        "    Perform a forward DNS lookup for a given hostname.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        return socket.gethostbyname(hostname)\n",
        "    except socket.gaierror:\n",
        "        return None\n",
        "\n",
        "def is_valid_bot(ip, ua):\n",
        "    \"\"\"\n",
        "    Determine if a request is from a legitimate search engine bot.\n",
        "    \"\"\"\n",
        "    if ip in verified_bot_ips:\n",
        "        return True\n",
        "    if ip in spoofed_bot_ips:\n",
        "        return False\n",
        "\n",
        "    for bot, suffixes in KNOWN_BOTS.items():\n",
        "        if bot.lower() in ua.lower():\n",
        "            rdns = reverse_dns(ip)\n",
        "            if not rdns or not any(rdns.endswith(sfx) for sfx in suffixes):\n",
        "                spoofed_bot_ips.add(ip)\n",
        "                return False\n",
        "            if forward_dns(rdns) != ip:\n",
        "                spoofed_bot_ips.add(ip)\n",
        "                return False\n",
        "            verified_bot_ips.add(ip)\n",
        "            return True\n",
        "\n",
        "    return False  # Not a known bot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sR9LTjwtMN_y",
        "outputId": "9167160c-3464-4820-e392-634f455d44fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting app/decoder.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app/decoder.py\n",
        "import os\n",
        "import re\n",
        "import sys\n",
        "import codecs\n",
        "import csv  # <<< Added for streaming\n",
        "import argparse\n",
        "import urllib.parse\n",
        "import base64\n",
        "# import pandas as pd # <<< Removed pandas dependency for streaming CSV\n",
        "from app.bot import is_valid_bot\n",
        "\n",
        "# Variables\n",
        "log_pattern = re.compile(\n",
        "    r'(?P<ip>\\S+) - - \\[(?P<time>[^\\]]+)\\] '\n",
        "    r'\"(?P<method>\\S+) (?P<url>\\S+) (?P<protocol>[^\"]+)\" '\n",
        "    r'(?P<status>\\d+) (?P<size>\\d+) '\n",
        "    r'\"(?P<referrer>[^\"]*)\" \"(?P<user_agent>[^\"]*)\" \"(?P<extra>[^\"]*)\"'\n",
        ")\n",
        "\n",
        "# Functions\n",
        "def esc_nl(text):\n",
        "    \"\"\"\n",
        "    Escape newline and carriage return characters in a string.\n",
        "    \"\"\"\n",
        "    return text.replace('\\n', '\\\\n').replace('\\r', '\\\\r').strip()\n",
        "\n",
        "\n",
        "def dec_url(text):\n",
        "    \"\"\"\n",
        "    Decode a URL-encoded string up to two iterations.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        first = urllib.parse.unquote(text)\n",
        "        if first == text:\n",
        "            return text\n",
        "\n",
        "        second = urllib.parse.unquote(first)\n",
        "        if second == first:\n",
        "            return first\n",
        "\n",
        "        return second\n",
        "    except Exception:\n",
        "        return text\n",
        "\n",
        "\n",
        "def dec_esc(text):\n",
        "    \"\"\"\n",
        "    Decode escaped character sequences such as \\\\xNN and \\\\uNNNN.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if '\\\\x' in text or '\\\\u' in text:\n",
        "            decoded = codecs.escape_decode(text.encode())[0].decode('utf-8', errors='replace')\n",
        "            return decoded\n",
        "        return text\n",
        "    except Exception:\n",
        "        return text\n",
        "\n",
        "\n",
        "def dec_base64(text):\n",
        "    \"\"\"\n",
        "    Detect and decode a Base64-encoded segment in the final URL path.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        last_part = text.rsplit(\"/\", 1)[-1]\n",
        "\n",
        "        # Heuristic: reasonably long, valid base64 characters\n",
        "        if re.fullmatch(r'[A-Za-z0-9+/=]{8,}', last_part):\n",
        "            decoded = base64.b64decode(last_part, validate=False).decode('utf-8', errors='ignore')\n",
        "            annotated = f\"{text}(base64:{decoded})\"\n",
        "            return annotated\n",
        "\n",
        "        return text\n",
        "    except Exception:\n",
        "        return text\n",
        "\n",
        "\n",
        "def dec_combined(text):\n",
        "    \"\"\"\n",
        "    Apply a sequence of decoding techniques to a string:\n",
        "    1. URL decoding (up to two iterations)\n",
        "    2. Escape sequence decoding (e.g., \\\\xNN, \\\\uNNNN)\n",
        "    3. Base64 decoding on the last URL path segment\n",
        "    \"\"\"\n",
        "    text = dec_url(text)\n",
        "    text = dec_esc(text)\n",
        "    text = dec_base64(text)\n",
        "    return text\n",
        "\n",
        "\n",
        "def parse_dec_line(line):\n",
        "    \"\"\"\n",
        "    Parse and decode a single NGINX access log line.\n",
        "    \"\"\"\n",
        "    match = log_pattern.match(line)\n",
        "    if not match:\n",
        "        return None, None  # Unparsable log line\n",
        "\n",
        "    fields = match.groupdict()\n",
        "\n",
        "    # Decode URL field (multi-step decoding)\n",
        "    fields['url'] = dec_combined(fields['url'])\n",
        "\n",
        "    # Decode referrer field (only take the decoded text, not flags)\n",
        "    fields['referrer'] = dec_combined(fields['referrer'])\n",
        "\n",
        "    # Apply newline escaping cleanup\n",
        "    for key in fields:\n",
        "        fields[key] = esc_nl(fields[key])\n",
        "\n",
        "    decoded = (\n",
        "        f'{fields[\"ip\"]} - - [{fields[\"time\"]}] '\n",
        "        f'\"{fields[\"method\"]} {fields[\"url\"]} {fields[\"protocol\"]}\" '\n",
        "        f'{fields[\"status\"]} {fields[\"size\"]} '\n",
        "        f'\"{fields[\"referrer\"]}\" \"{fields[\"user_agent\"]}\" \"{fields[\"extra\"]}\"'\n",
        "    )\n",
        "\n",
        "    return decoded, fields\n",
        "\n",
        "\n",
        "def parse_dec_file(in_path, out_path):\n",
        "    \"\"\"\n",
        "    Decode and clean all entries in a log file and write the results to a new file.\n",
        "    \"\"\"\n",
        "    with open(in_path, 'r', encoding='utf-8', errors='replace') as in_file, \\\n",
        "        open(out_path, 'w', encoding='utf-8') as out_file:\n",
        "\n",
        "        for line in in_file:\n",
        "            decoded, fields = parse_dec_line(line)\n",
        "\n",
        "            if not fields:\n",
        "                continue  # Skip unparsed line\n",
        "\n",
        "            if is_valid_bot(fields['ip'], fields['user_agent']):\n",
        "                continue  # Skip valid bot\n",
        "\n",
        "            out_file.write(f\"{decoded}\\n\")\n",
        "\n",
        "\n",
        "# Hapus parse_dec_file_to_dataframe dan parse_dec_file_to_csv (lama)\n",
        "# Ganti dengan fungsi streaming yang baru\n",
        "def parse_dec_file_stream_to_csv(in_path, out_path):\n",
        "    \"\"\"\n",
        "    Decode and clean all entries in a log file and stream them to a CSV file.\n",
        "    This avoids loading all data into RAM (MemoryError).\n",
        "    \"\"\"\n",
        "    print(\"‚è≥ Starting streaming decode to CSV...\")\n",
        "    \n",
        "    with open(in_path, 'r', encoding='utf-8', errors='replace') as in_file, \\\n",
        "         open(out_path, 'w', encoding='utf-8', newline='') as out_file:\n",
        "\n",
        "        writer = None\n",
        "        \n",
        "        for no, line in enumerate(in_file, 1):\n",
        "            _, fields = parse_dec_line(line)\n",
        "\n",
        "            if not fields:\n",
        "                continue  # Skip unparsed line\n",
        "\n",
        "            if is_valid_bot(fields['ip'], fields['user_agent']):\n",
        "                continue  # Skip valid bot\n",
        "\n",
        "            # Add line number (optional, but in the original code)\n",
        "            fields['no'] = no\n",
        "            \n",
        "            # Initialize CSV writer after the first record is processed (to get fieldnames)\n",
        "            if writer is None:\n",
        "                fieldnames = list(fields.keys())\n",
        "                writer = csv.DictWriter(out_file, fieldnames=fieldnames)\n",
        "                writer.writeheader()\n",
        "                \n",
        "            writer.writerow(fields)\n",
        "            \n",
        "            # Progress update (optional)\n",
        "            if no % 100000 == 0:\n",
        "                print(f\"  Processed {no:,} lines...\")\n",
        "\n",
        "    print(\"‚úÖ Streaming decode complete.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser(description=\"NGINX log decoder.\")\n",
        "    parser.add_argument(\"in_file\", help=\"NGINX log file\")\n",
        "    parser.add_argument(\"out_file\", help=\"The decoded NGINX log file\")\n",
        "    parser.add_argument(\"--csv\", action=\"store_true\", help=\"Save the output in CSV format.\")\n",
        "\n",
        "    # Parse the arguments\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    if not os.path.exists(args.in_file):\n",
        "        print(f\"‚ùå File not found: '{args.in_file}'\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    out_dir = os.path.dirname(args.out_file)\n",
        "    if out_dir and not os.path.exists(out_dir):\n",
        "        # NOTE: Instead of checking existence, better to create it if it doesn't exist\n",
        "        try:\n",
        "            os.makedirs(out_dir, exist_ok=True)\n",
        "        except OSError as e:\n",
        "            print(f\"‚ùå Cannot create output directory: '{out_dir}' - {e}\")\n",
        "            sys.exit(1)\n",
        "        \n",
        "    if args.csv:\n",
        "        # Use the memory-efficient streaming function\n",
        "        parse_dec_file_stream_to_csv(args.in_file, args.out_file)\n",
        "    else:\n",
        "        parse_dec_file(args.in_file, args.out_file)\n",
        "\n",
        "    print(f\"‚úÖ Log file successfully converted to {args.out_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uivwcyzrMRRU",
        "outputId": "a3fc95c3-a5d2-48ff-fd01-6071b13882f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting app/main.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app/main.py\n",
        "# app/main.py\n",
        "import os\n",
        "import re\n",
        "import csv\n",
        "import sys\n",
        "import argparse\n",
        "import warnings\n",
        "import time \n",
        "from collections import Counter\n",
        "from urllib.parse import urlparse, unquote\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import BertTokenizerFast, BertModel\n",
        "from sklearn.preprocessing import normalize\n",
        "from umap import UMAP # <--- TAMBAHKAN IMPORT INI\n",
        "\n",
        "# =========================\n",
        "# IMPORT METRIK BARU (DBCV)\n",
        "# =========================\n",
        "try:\n",
        "    from hdbscan.validity import validity_index as dbcv_score\n",
        "except ImportError:\n",
        "    print(\"=\"*50)\n",
        "    print(\"‚ùå ERROR: Library 'hdbscan' tidak ditemukan.\")\n",
        "    print(\"Silakan install dengan: pip install hdbscan\")\n",
        "    print(\"=\"*50)\n",
        "    sys.exit(1)\n",
        "\n",
        "\n",
        "# =========================\n",
        "# IMPORT KHUSUS GPU (CUML/CUPY) DAN FALLBACK KE SKLEARN (CPU)\n",
        "# =========================\n",
        "try:\n",
        "    from cuml.cluster import OPTICS as CumlOPTICS\n",
        "    import cupy as cp\n",
        "    \n",
        "    if cp.cuda.is_available():\n",
        "        HAS_GPU_CLUSTERING = True\n",
        "        OPTICS_IMPL = CumlOPTICS\n",
        "    else:\n",
        "        raise RuntimeError(\"CuPy terinstal tetapi GPU tidak terdeteksi atau siap.\")\n",
        "\n",
        "except (ImportError, RuntimeError, Exception) as e: \n",
        "    from sklearn.cluster import OPTICS as SklearnOPTICS\n",
        "    OPTICS_IMPL = SklearnOPTICS\n",
        "    cp = np \n",
        "    HAS_GPU_CLUSTERING = False\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Konfigurasi & Inisialisasi\n",
        "# =========================\n",
        "\n",
        "warnings.filterwarnings(\n",
        "    \"ignore\",\n",
        "    message=\"`clean_up_tokenization_spaces` was not set\",\n",
        "    category=FutureWarning,\n",
        ")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "if HAS_GPU_CLUSTERING and device.type == \"cuda\":\n",
        "    print(\"‚úÖ GPU Acceleration aktif untuk BERT dan OPTICS (via CuML).\")\n",
        "elif HAS_GPU_CLUSTERING and device.type == \"cpu\":\n",
        "    print(\"‚ö†Ô∏è Peringatan: CuML terinstal, tetapi PyTorch menggunakan CPU. Klastering GPU tidak efektif.\")\n",
        "else:\n",
        "    print(\"‚ùå GPU Clustering (CuML) tidak terinstal. Menggunakan Scikit-learn (CPU).\")\n",
        "\n",
        "\n",
        "TOKENIZER = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
        "torch_dtype = torch.float16 if device.type == \"cuda\" else torch.float32\n",
        "MODEL = BertModel.from_pretrained(\"bert-base-uncased\", torch_dtype=torch_dtype).to(device)\n",
        "MODEL.eval()\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Utilitas parsing & masking (Tidak Berubah)\n",
        "# =========================\n",
        "\n",
        "def split_url_tokens(url: str):\n",
        "    parsed = urlparse(url)\n",
        "    path = unquote(parsed.path or \"\")\n",
        "    query = unquote(parsed.query or \"\")\n",
        "    delimiters = r\"[\\/\\-\\_\\=\\&\\?\\.\\+\\(\\)\\[\\]\\<\\>\\{\\}]\"\n",
        "    tokens = re.split(delimiters, path.strip(\"/\")) + re.split(delimiters, query)\n",
        "    return [tok for tok in tokens if tok]\n",
        "\n",
        "\n",
        "def iter_urls_from_decoded_csv(csv_path: str, mask_numbers: bool = True):\n",
        "    with open(csv_path, \"r\", encoding=\"utf-8\", errors=\"ignore\", newline=\"\") as f:\n",
        "        reader = csv.DictReader(f)\n",
        "        if \"url\" not in reader.fieldnames:\n",
        "            raise ValueError(\n",
        "                f\"CSV {csv_path} tidak memiliki kolom 'url'. \"\n",
        "                \"Pastikan file dibuat oleh app.decoder dengan flag --csv.\"\n",
        "            )\n",
        "        for row in reader:\n",
        "            url = row[\"url\"]\n",
        "            if mask_numbers:\n",
        "                url = re.sub(r\"\\d+\", \"<NUM>\", url)\n",
        "            yield url\n",
        "\n",
        "\n",
        "def topk_urls(csv_path: str, k: int = 200_000):\n",
        "    cnt = Counter()\n",
        "    for url in iter_urls_from_decoded_csv(csv_path):\n",
        "        cnt[url] += 1\n",
        "    return [u for u, _ in cnt.most_common(k)]\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Embedding (memmap, per-batch) (Tidak Berubah)\n",
        "# =========================\n",
        "\n",
        "def generate_url_embeddings_to_memmap(\n",
        "    url_list,\n",
        "    memmap_path: str,\n",
        "    batch_size: int = 128,\n",
        "    max_length: int = 64,\n",
        "    verbose: bool = False,\n",
        "):\n",
        "    n = len(url_list)\n",
        "    if n == 0:\n",
        "        raise ValueError(\"url_list kosong ‚Äî tidak ada URL untuk di-embed.\")\n",
        "\n",
        "    out_dir = os.path.dirname(memmap_path)\n",
        "    if out_dir:\n",
        "        os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    hidden_size = MODEL.config.hidden_size\n",
        "    embs = np.memmap(memmap_path, dtype=\"float32\", mode=\"w+\", shape=(n, hidden_size))\n",
        "    \n",
        "    start_time = time.time()\n",
        "    i = 0\n",
        "    batch_count = 0\n",
        "    total_batches = (n + batch_size - 1) // batch_size\n",
        "    \n",
        "    while i < n:\n",
        "        j = min(i + batch_size, n)\n",
        "        batch = url_list[i:j]\n",
        "        \n",
        "        batch_count += 1\n",
        "        \n",
        "        inputs = TOKENIZER(\n",
        "            batch,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=max_length,\n",
        "        ).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = MODEL(**inputs)\n",
        "            batch_emb = outputs.last_hidden_state.mean(dim=1)\n",
        "            if batch_emb.dtype != torch.float32:\n",
        "                batch_emb = batch_emb.float()\n",
        "\n",
        "        embs[i:j, :] = batch_emb.detach().cpu().numpy()\n",
        "        i = j\n",
        "\n",
        "        if verbose and batch_count % 10 == 0 or i == n:\n",
        "            elapsed = time.time() - start_time\n",
        "            rate = i / elapsed if elapsed > 0 else 0\n",
        "            remaining_seconds = (n - i) / rate if rate > 0 else 0\n",
        "            \n",
        "            if remaining_seconds >= 3600:\n",
        "                eta_str = f\"{remaining_seconds/3600:.1f}h\"\n",
        "            elif remaining_seconds >= 60:\n",
        "                eta_str = f\"{remaining_seconds/60:.1f}m\"\n",
        "            else:\n",
        "                eta_str = f\"{remaining_seconds:.1f}s\"\n",
        "                \n",
        "            print(\n",
        "                f\"[BATCH {batch_count}/{total_batches}] Embedded: {i}/{n} | Rate: {rate:.1f} url/s | ETA: {eta_str}\",\n",
        "                end='\\r' if i < n else '\\n',\n",
        "                flush=True\n",
        "            )\n",
        "\n",
        "    embs.flush() \n",
        "    del embs\n",
        "    return memmap_path\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Evaluasi Metriks (HANYA EUCLIDEAN)\n",
        "# =========================\n",
        "\n",
        "def evaluate_clustering_metrics(embs: np.ndarray, labels: np.ndarray):\n",
        "    \"\"\"Hitung metrik evaluasi klastering (DBCV) menggunakan 'euclidean'.\"\"\"\n",
        "    \n",
        "    # Konversi ke float64 (double) untuk kompatibilitas hdbscan/cython\n",
        "    if embs.dtype != np.float64:\n",
        "        embs = embs.astype(np.float64)\n",
        "    # --- AKHIR TAMBAHAN ---\n",
        "\n",
        "    # Atur ke 0 untuk evaluasi penuh.\n",
        "    # Atur > 0 (misal 20000) untuk sampling jika data besar.\n",
        "    MAX_SAMPLES_FOR_EVAL = 0 \n",
        "    \n",
        "    clustered_indices = labels != -1\n",
        "    X_clustered = embs[clustered_indices]\n",
        "    labels_clustered = labels[clustered_indices]\n",
        "\n",
        "    n_clusters_total = len(np.unique(labels)) \n",
        "    n_outliers = np.sum(labels == -1)\n",
        "    \n",
        "    unique_clustered_labels = np.unique(labels_clustered)\n",
        "    n_clusters_actual = len(unique_clustered_labels)\n",
        "    n_clustered_points = X_clustered.shape[0]\n",
        "\n",
        "    if n_clusters_actual < 2:\n",
        "        print(f\"‚ö†Ô∏è Hanya {n_clusters_actual} klaster valid ditemukan (di luar outlier).\")\n",
        "        print(\"‚ö†Ô∏è DBCV (validity_index) memerlukan setidaknya 2 klaster.\")\n",
        "        return {\n",
        "            \"N_URLS\": len(labels),\n",
        "            \"N_CLUSTERED_POINTS\": int(n_clustered_points), # TAMBAHKAN INI\n",
        "            \"N_CLUSTERS_TOTAL\": n_clusters_total,\n",
        "            \"N_CLUSTERS_ACTUAL\": n_clusters_actual,\n",
        "            \"N_OUTLIERS\": int(n_outliers),\n",
        "            \"DBCV_SCORE\": np.nan, \n",
        "            \"EVAL_SAMPLE_SIZE\": 0,\n",
        "        }\n",
        "\n",
        "    # --- LOGIKA SAMPLING ---\n",
        "    if MAX_SAMPLES_FOR_EVAL > 0 and n_clustered_points > MAX_SAMPLES_FOR_EVAL:\n",
        "        print(f\"‚ö†Ô∏è  Jumlah data terklaster ({n_clustered_points:,}) terlalu besar untuk evaluasi penuh.\")\n",
        "        print(f\"Sampling acak {MAX_SAMPLES_FOR_EVAL:,} titik untuk menghitung DBCV...\")\n",
        "        \n",
        "        sample_indices = np.random.choice(n_clustered_points, MAX_SAMPLES_FOR_EVAL, replace=False)\n",
        "        X_eval = X_clustered[sample_indices]\n",
        "        labels_eval = labels_clustered[sample_indices]\n",
        "        eval_sample_size = MAX_SAMPLES_FOR_EVAL\n",
        "    else:\n",
        "        # Lakukan evaluasi penuh\n",
        "        X_eval = X_clustered\n",
        "        labels_eval = labels_clustered\n",
        "        eval_sample_size = n_clustered_points\n",
        "    # --- AKHIR LOGIKA SAMPLING ---\n",
        "\n",
        "    print(f\"üìä Menghitung DBCV (euclidean) pada {eval_sample_size:,} titik data...\")\n",
        "    eval_start_time = time.time()\n",
        "    try:\n",
        "        # Hardcode metric 'euclidean'\n",
        "        dbcv = dbcv_score(X_eval, labels_eval, metric='euclidean')\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error saat menghitung DBCV: {e}\")\n",
        "        dbcv = np.nan\n",
        "    \n",
        "    eval_elapsed = time.time() - eval_start_time\n",
        "    print(f\"‚úÖ Perhitungan DBCV selesai dalam {eval_elapsed:.2f} detik.\")\n",
        "\n",
        "    \n",
        "    return {\n",
        "        \"N_URLS\": len(labels),\n",
        "        \"N_CLUSTERED_POINTS\": int(n_clustered_points), # TAMBAHKAN INI JUGA\n",
        "        \"N_CLUSTERS_TOTAL\": n_clusters_total,\n",
        "        \"N_CLUSTERS_ACTUAL\": n_clusters_actual,\n",
        "        \"N_OUTLIERS\": int(n_outliers),\n",
        "        \"DBCV_SCORE\": dbcv, \n",
        "        \"EVAL_SAMPLE_SIZE\": int(eval_sample_size),\n",
        "    }\n",
        "\n",
        "# =========================\n",
        "# Pipeline utama\n",
        "# =========================\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser(\n",
        "        description=\"Cluster NGINX URLs with BERT + OPTICS (RAM-friendly).\"\n",
        "    )\n",
        "    # ... (Argumen) ...\n",
        "    parser.add_argument(\n",
        "        \"in_file\",\n",
        "        help=\"Input CSV hasil decoder (outputs/access_decoded.csv). Gunakan --decoded-csv.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"out_file\",\n",
        "        nargs=\"?\" if \"--evaluate\" in sys.argv else None, \n",
        "        help=\"Output CSV berisi label cluster (mis. outputs/access_optics.csv). Wajib jika tidak --evaluate.\",\n",
        "    )\n",
        "    parser.add_argument(\"-m\", type=int, default=5, help=\"OPTICS min_samples\")\n",
        "    parser.add_argument(\"-e\", type=float, default=0.5, help=\"OPTICS eps\")\n",
        "    parser.add_argument(\n",
        "        \"--topk\",\n",
        "        type=int,\n",
        "        default=200_000,\n",
        "        help=\"Ambil Top-K URL paling sering (semakin kecil semakin hemat RAM).\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--batch-size\", type=int, default=128, help=\"Batch size embedding BERT\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--max-length\",\n",
        "        type=int,\n",
        "        default=64,\n",
        "        help=\"Maks panjang token untuk BERT (64 cukup untuk mayoritas URL).\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--memmap\",\n",
        "        default=\"outputs/embeddings.dat\",\n",
        "        help=\"Path file memmap untuk embeddings (akan dibuat).\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--decoded-csv\",\n",
        "        action=\"store_true\",\n",
        "        help=\"Wajib diaktifkan: menandakan input adalah CSV hasil decoder.\",\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--evaluate\",\n",
        "        action=\"store_true\",\n",
        "        help=\"Mode Evaluasi: Hanya menghitung metrik (DBCV) dan menyimpannya ke file .txt, tidak menyimpan .csv cluster.\",\n",
        "    )\n",
        "    \n",
        "    # --- HAPUS ARGUMEN --metric ---\n",
        "    \n",
        "    parser.add_argument(\n",
        "        \"-v\",\n",
        "        \"--verbose\",\n",
        "        action=\"store_true\",\n",
        "        help=\"Tampilkan rincian progress embedding, termasuk ETA dan rate.\",\n",
        "    )\n",
        "    # --- TAMBAHAN ARGUMEN UMAP ---\n",
        "    parser.add_argument(\"--umap-dim\", type=int, default=0, help=\"Jalankan reduksi dimensi UMAP ke N dimensi (misal 20) sebelum clustering. 0 = nonaktifkan.\")\n",
        "    parser.add_argument(\"--umap-nn\", type=int, default=15, help=\"Parameter n_neighbors untuk UMAP (mempengaruhi seberapa 'global' struktur yang dilihat).\")\n",
        "    # --- AKHIR TAMBAHAN ---\n",
        "\n",
        "    parser.add_argument( \n",
        "        \"-j\",\n",
        "        \"--n-jobs\",\n",
        "        type=int,\n",
        "        default=-1 if not HAS_GPU_CLUSTERING else 1, \n",
        "        help=\"Jumlah core CPU yang digunakan untuk OPTICS (-1 untuk semua core). Diabaikan oleh CuML.\",\n",
        "    ) \n",
        "\n",
        "\n",
        "    args = parser.parse_args()\n",
        "    \n",
        "    # --- VALIDASI ARGUMEN ---\n",
        "    if not args.evaluate and not args.out_file:\n",
        "        parser.error(\"argumen 'out_file' wajib diberikan jika --evaluate tidak aktif.\")\n",
        "        \n",
        "    if not os.path.exists(args.in_file):\n",
        "        print(f\"‚ùå File tidak ditemukan: '{args.in_file}'\")\n",
        "        sys.exit(1)\n",
        "        \n",
        "    if args.out_file:\n",
        "        out_dir = os.path.dirname(args.out_file)\n",
        "        if out_dir and not os.path.exists(out_dir):\n",
        "            os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    if not args.decoded_csv:\n",
        "        print(\n",
        "            \"‚ùå Harap gunakan --decoded-csv dan berikan input CSV dari app.decoder.\"\n",
        "        )\n",
        "        sys.exit(1)\n",
        "        \n",
        "    \n",
        "    if args.evaluate:\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"         üåü MODE EVALUASI METRIK AKTIF üåü\")\n",
        "        print(\"    Metrik Jarak: euclidean (pada data L2-Norm, setara cosine)\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "\n",
        "    # 1) Top-K URL (streaming)\n",
        "    print(f\"üîé Menghitung Top-K ({args.topk}) URL dari decoded CSV (streaming)...\")\n",
        "    unique_urls = topk_urls(args.in_file, k=args.topk)\n",
        "    print(f\"‚úÖ Siap embed: {len(unique_urls):,} URL\")\n",
        "\n",
        "    if len(unique_urls) == 0:\n",
        "        print(\"‚ùå Tidak ada URL yang ditemukan. Periksa input CSV.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    # 2) Tokenisasi ringan string URL \n",
        "    print(\"üî§ Menyusun token string URL (ringan)...\")\n",
        "    tokenized_urls = [\" \".join(split_url_tokens(u)) for u in unique_urls]\n",
        "\n",
        "    # 3) Embedding BERT ‚Üí memmap (hemat RAM)\n",
        "    print(\"üß† Menghitung embedding BERT (stream ke memmap)...\")\n",
        "    memmap_path = generate_url_embeddings_to_memmap(\n",
        "        tokenized_urls,\n",
        "        args.memmap,\n",
        "        batch_size=args.batch_size,\n",
        "        max_length=args.max_length,\n",
        "        verbose=args.verbose, \n",
        "    )\n",
        "\n",
        "    # 4) Load memmap readonly, normalisasi, kemudian OPTICS\n",
        "    # INI ADALAH KUNCI: Normalisasi (L2-norm)\n",
        "    print(\"‚ú® Normalisasi vektor (L2-norm)...\") \n",
        "    hidden_size = MODEL.config.hidden_size\n",
        "    embs = np.memmap(\n",
        "        memmap_path,\n",
        "        dtype=\"float32\",\n",
        "        mode=\"r\",\n",
        "        shape=(len(tokenized_urls), hidden_size),\n",
        "    )\n",
        "    \n",
        "    embs_normalized = normalize(embs) # NumPy Array (CPU)\n",
        "    del embs \n",
        "\n",
        "    # --- BLOK UMAP BARU ---\n",
        "    data_to_cluster = embs_normalized # Defaultnya data 768-D\n",
        "\n",
        "    if args.umap_dim > 0:\n",
        "        print(f\"üó∫Ô∏è  Menjalankan UMAP (Reduksi Dimensi) dari 768 ke {args.umap_dim} dimensi...\")\n",
        "        print(f\"(UMAP n_neighbors={args.umap_nn}, n_jobs={args.n_jobs})\")\n",
        "        umap_start_time = time.time()\n",
        "        reducer = UMAP(\n",
        "            n_components=args.umap_dim,\n",
        "            n_neighbors=args.umap_nn,\n",
        "            min_dist=0.1, \n",
        "            metric='euclidean', \n",
        "            random_state=42,\n",
        "            n_jobs=args.n_jobs, # Gunakan n_jobs untuk UMAP\n",
        "            verbose=args.verbose, # Tampilkan progress UMAP jika verbose\n",
        "        )\n",
        "        data_to_cluster = reducer.fit_transform(embs_normalized)\n",
        "        del embs_normalized # Hapus data 768-D\n",
        "        umap_elapsed = time.time() - umap_start_time\n",
        "        print(f\"‚úÖ UMAP Selesai dalam {umap_elapsed:.2f} detik.\")\n",
        "    # --- AKHIR BLOK UMAP ---\n",
        "\n",
        "    # --- PENGELOLAAN DATA DAN KLATERING UNTUK GPU/CPU ---\n",
        "    \n",
        "    embs_device = data_to_cluster \n",
        "    if HAS_GPU_CLUSTERING:\n",
        "        print(\"üöÄ Transfer data ke GPU (CuPy)...\")\n",
        "        embs_device = cp.asarray(embs_normalized)\n",
        "    \n",
        "    # Hardcode metric ke 'euclidean'\n",
        "    print(f\"üß© OPTICS clustering (min_samples={args.m}, eps={args.e}, metric='euclidean')...\")\n",
        "    \n",
        "    clustering_start_time = time.time()\n",
        "    \n",
        "    optics_params = {\n",
        "        'min_samples': args.m,\n",
        "        'eps': args.e,\n",
        "        'metric': 'euclidean', # Hardcode 'euclidean'\n",
        "    }\n",
        "    \n",
        "    if not HAS_GPU_CLUSTERING: # Logika untuk CPU / Sklearn\n",
        "        optics_params['n_jobs'] = args.n_jobs\n",
        "        # Selalu gunakan 'ball_tree' karena kita selalu 'euclidean'\n",
        "        print(\"üí° Menggunakan algorithm='ball_tree' untuk klastering CPU cepat.\")\n",
        "        optics_params['algorithm'] = 'ball_tree'\n",
        "    \n",
        "    optics = OPTICS_IMPL(**optics_params)\n",
        "    labels_device = optics.fit_predict(embs_device)\n",
        "    \n",
        "    clustering_elapsed = time.time() - clustering_start_time\n",
        "\n",
        "    if HAS_GPU_CLUSTERING:\n",
        "        labels = cp.asnumpy(labels_device)\n",
        "        del embs_device\n",
        "    else:\n",
        "        labels = labels_device\n",
        "        \n",
        "    print(f\"‚úÖ OPTICS selesai dalam {clustering_elapsed:.2f} detik.\")\n",
        "    # --- AKHIR PENGELOLAAN DATA ---\n",
        "\n",
        "\n",
        "    # 5) Mode Evaluasi: Hitung metrik, simpan ke TXT, lalu keluar\n",
        "    if args.evaluate:\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(f\"         üìä MENGHITUNG METRIK EVALUASI (Metrik: euclidean) üìâ\")\n",
        "        print(\"=\"*50)\n",
        "        \n",
        "        # Panggil tanpa parameter metric\n",
        "        metrics = evaluate_clustering_metrics(data_to_cluster, labels)\n",
        "        \n",
        "        # Tentukan nama file output evaluasi\n",
        "        if args.out_file:\n",
        "            base_name = os.path.splitext(args.out_file)[0]\n",
        "            eval_txt_path = f\"{base_name}.eval.txt\"\n",
        "        else:\n",
        "            os.makedirs(\"outputs\", exist_ok=True) \n",
        "            # Tambahkan info UMAP ke nama file default\n",
        "            umap_str = f\"umap{args.umap_dim}nn{args.umap_nn}\" if args.umap_dim > 0 else \"no_umap\"\n",
        "            eval_txt_path = f\"outputs/evaluation_results.{umap_str}.m{args.m}.e{args.e}.txt\"\n",
        "\n",
        "        print(f\"üíæ Menyimpan hasil evaluasi ke: {eval_txt_path}...\")\n",
        "        with open(eval_txt_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(\"=\"*50 + \"\\n\"); f.write(\" üìà HASIL METRIK KLATERING üìâ\\n\"); f.write(\"=\"*50 + \"\\n\")\n",
        "            f.write(f\"Input File: {args.in_file}\\n\")\n",
        "            f.write(f\"Total URL diproses: {metrics['N_URLS']:,}\\n\")\n",
        "            \n",
        "            if args.umap_dim > 0:\n",
        "                f.write(f\"Waktu UMAP: {umap_elapsed:.2f} detik\\n\")\n",
        "            f.write(f\"Waktu Klastering OPTICS: {clustering_elapsed:.2f} detik\\n\")\n",
        "            \n",
        "            if args.umap_dim > 0:\n",
        "                f.write(f\"Reduksi Dimensi: UMAP ke {args.umap_dim}-D (n_neighbors={args.umap_nn})\\n\")\n",
        "            else:\n",
        "                f.write(\"Reduksi Dimensi: Tidak ada (768-D)\\n\")\n",
        "\n",
        "            f.write(f\"Metrik Jarak: euclidean (pada data L2-Norm)\\n\") \n",
        "            f.write(f\"OPTICS min_samples: {args.m}, eps: {args.e}\\n\")\n",
        "            f.write(\"-\" * 50 + \"\\n\")\n",
        "            f.write(f\"Jumlah Klaster Nyata (selain -1): {metrics['N_CLUSTERS_ACTUAL']}\\n\")\n",
        "            f.write(f\"Jumlah Outlier (Label -1): {metrics['N_OUTLIERS']:,}\\n\")\n",
        "            \n",
        "            if metrics[\"EVAL_SAMPLE_SIZE\"] < metrics[\"N_CLUSTERED_POINTS\"]:\n",
        "                 f.write(f\"Evaluasi Dihitung Pada: {metrics['EVAL_SAMPLE_SIZE']:,} sampel acak\\n\")\n",
        "            else:\n",
        "                 f.write(f\"Evaluasi Dihitung Pada: {metrics['EVAL_SAMPLE_SIZE']:,} titik (evaluasi penuh data terklaster)\\n\")\n",
        "            f.write(\"-\" * 50 + \"\\n\")\n",
        "            \n",
        "            if not np.isnan(metrics['DBCV_SCORE']):\n",
        "                f.write(f\"DBCV Score (Ideal: mendekati 1.0): {metrics['DBCV_SCORE']:.4f}\\n\")\n",
        "            else:\n",
        "                f.write(f\"DBCV Score: {metrics['DBCV_SCORE']}\\n\")\n",
        "            f.write(\"=\"*50 + \"\\n\")\n",
        "        \n",
        "        print(f\"‚úÖ Evaluasi disimpan. Skrip berhenti (mode --evaluate).\")\n",
        "        del data_to_cluster\n",
        "        sys.exit(0) \n",
        "\n",
        "    # Hapus data yang dikluster untuk hemat RAM\n",
        "    del data_to_cluster \n",
        "    \n",
        "    print(f\"üíæ Menyimpan hasil label (CSV) ke: {args.out_file}...\")\n",
        "    df_label = pd.DataFrame({\"masked\": unique_urls, \"cluster\": labels}).sort_values(by=\"cluster\")\n",
        "    df_label.to_csv(args.out_file, index=False, encoding=\"utf-8\")\n",
        "    print(f\"‚úÖ File CSV berhasil disimpan.\")\n",
        "\n",
        "    txt_path = f\"{args.out_file}.txt\"\n",
        "    unique_labels = sorted(set(labels), key=lambda x: (x == -1, x))\n",
        "    groups = {lab: [] for lab in unique_labels}\n",
        "    for i, lab in enumerate(labels):\n",
        "        groups[lab].append(unique_urls[i])\n",
        "    print(f\"üìù Menyimpan cluster listing (TXT) ke: {txt_path}...\")\n",
        "    with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for lab in unique_labels:\n",
        "            title = \"Noise (-1)\" if lab == -1 else f\"Cluster {lab}\"\n",
        "            f.write(f\"\\n{title} ({len(groups[lab])} items):\\n\")\n",
        "            for u in groups[lab]:\n",
        "                f.write(f\"  {u}\\n\")\n",
        "    print(f\"‚úÖ File TXT berhasil disimpan.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVMX34wQHAdr",
        "outputId": "7f371efe-3d82-4562-87fc-724710714145"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10365152\n"
          ]
        }
      ],
      "source": [
        "!python -c \"print(sum(1 for _ in open('inputs/access.log', encoding='utf-8', errors='ignore')))\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "9283833\n"
          ]
        }
      ],
      "source": [
        "!python -c \"print(sum(1 for _ in open('outputs/access_decoded.csv', encoding='utf-8', errors='ignore')))\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚è≥ Starting streaming decode to CSV...\n",
            "‚úÖ Streaming decode complete.\n",
            "‚úÖ Log file successfully converted to outputs/sample_decoded.csv\n"
          ]
        }
      ],
      "source": [
        "!.\\.venv\\Scripts\\python -m app.decoder inputs/sample.log outputs/sample_decoded.csv --csv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vuWKnEcgMZh-",
        "outputId": "37f7a5ca-df61-4026-aae8-3649e7582b16"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "UsageError: Cell magic `%%powershell` not found.\n"
          ]
        }
      ],
      "source": [
        "!.\\.venv\\Scripts\\python -u -m app.main outputs/access_decoded.csv outputs/access_optics2.csv -m 10 --topk 200000 --batch-size 128 --max-length 64 --decoded-csv --metric euclidean -v"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Report tersimpan: outputs/cluster_report.html\n"
          ]
        }
      ],
      "source": [
        "!.\\.venv\\Scripts\\python -m app.report outputs/access_optics.csv outputs/cluster_report.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting app/expand_originals.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app/expand_originals.py\n",
        "import os\n",
        "import re\n",
        "import csv\n",
        "import argparse\n",
        "import sqlite3\n",
        "from collections import defaultdict, Counter\n",
        "from html import escape\n",
        "import math\n",
        "\n",
        "MASK_RE = re.compile(r\"\\d+\")\n",
        "\n",
        "def mask_url(url: str) -> str:\n",
        "    return MASK_RE.sub(\"<NUM>\", url or \"\")\n",
        "\n",
        "def load_clustered_masked(masked_cluster_csv):\n",
        "    masked_to_cluster = {}\n",
        "    cluster_to_masked = defaultdict(set)\n",
        "    with open(masked_cluster_csv, \"r\", encoding=\"utf-8\", newline=\"\") as f:\n",
        "        r = csv.DictReader(f)\n",
        "        if \"masked\" not in r.fieldnames or \"cluster\" not in r.fieldnames:\n",
        "            raise ValueError(\"CSV cluster harus punya kolom 'masked' dan 'cluster'\")\n",
        "        for row in r:\n",
        "            m = row[\"masked\"]\n",
        "            c = int(row[\"cluster\"])\n",
        "            masked_to_cluster[m] = c\n",
        "            cluster_to_masked[c].add(m)\n",
        "    return masked_to_cluster, cluster_to_masked\n",
        "\n",
        "# -------- Aggregator per original URL ‚Üí per-IP buckets --------\n",
        "class OrigAgg:\n",
        "    __slots__ = (\"total\", \"ip_cnt\", \"ip_sample\")\n",
        "    def __init__(self):\n",
        "        self.total = 0\n",
        "        self.ip_cnt = Counter()\n",
        "        self.ip_sample = {}  # ip -> first-seen row dict\n",
        "\n",
        "def format_log_line(row: dict) -> str:\n",
        "    def g(k, default=\"\"):\n",
        "        v = row.get(k, default) if row else default\n",
        "        return \"\" if v is None else str(v)\n",
        "    ip = g(\"ip\", \"-\")\n",
        "    time = g(\"time\", \"-\")\n",
        "    method = g(\"method\", \"-\")\n",
        "    url = g(\"url\", \"-\")\n",
        "    protocol = g(\"protocol\", \"-\")\n",
        "    status = g(\"status\", \"-\")\n",
        "    size = g(\"size\", \"-\")\n",
        "    ref = g(\"referrer\", \"-\")\n",
        "    ua = g(\"user_agent\", \"-\")\n",
        "    extra = g(\"extra\", \"-\")\n",
        "    return f'{ip} - - [{time}] \"{method} {url} {protocol}\" {status} {size} \"{ref}\" \"{ua}\" \"{extra}\"'\n",
        "\n",
        "def stream_count_originals(decoded_csv, masked_to_cluster,\n",
        "                            max_originals_per_masked=200,\n",
        "                            top_ips_per_original=3):\n",
        "    masked_counts = Counter()\n",
        "    store: dict[str, dict[str, OrigAgg]] = defaultdict(dict)\n",
        "\n",
        "    with open(decoded_csv, \"r\", encoding=\"utf-8\", errors=\"ignore\", newline=\"\") as f:\n",
        "        r = csv.DictReader(f)\n",
        "        need = {\"ip\",\"time\",\"method\",\"url\",\"protocol\",\"status\",\"size\",\"referrer\",\"user_agent\",\"extra\"}\n",
        "        if not need.issubset(set(r.fieldnames or [])):\n",
        "            raise ValueError(\"CSV decoded harus punya kolom: \" + \", \".join(sorted(need)))\n",
        "\n",
        "        for row in r:\n",
        "            orig = row[\"url\"]\n",
        "            m = mask_url(orig)\n",
        "            if m not in masked_to_cluster:\n",
        "                continue\n",
        "\n",
        "            masked_counts[m] += 1\n",
        "            bucket = store[m]\n",
        "\n",
        "            if orig not in bucket and len(bucket) >= max_originals_per_masked:\n",
        "                top = sorted(bucket.items(), key=lambda kv: kv[1].total, reverse=True)[:max_originals_per_masked]\n",
        "                bucket.clear()\n",
        "                bucket.update(top)\n",
        "\n",
        "            agg = bucket.get(orig)\n",
        "            if agg is None:\n",
        "                agg = OrigAgg()\n",
        "                bucket[orig] = agg\n",
        "\n",
        "            agg.total += 1\n",
        "            ip = row.get(\"ip\") or \"-\"\n",
        "            agg.ip_cnt[ip] += 1\n",
        "            if ip not in agg.ip_sample:  # first-seen per IP\n",
        "                agg.ip_sample[ip] = {\n",
        "                    \"ip\": row.get(\"ip\"),\n",
        "                    \"time\": row.get(\"time\"),\n",
        "                    \"method\": row.get(\"method\"),\n",
        "                    \"url\": row.get(\"url\"),\n",
        "                    \"protocol\": row.get(\"protocol\"),\n",
        "                    \"status\": row.get(\"status\"),\n",
        "                    \"size\": row.get(\"size\"),\n",
        "                    \"referrer\": row.get(\"referrer\"),\n",
        "                    \"user_agent\": row.get(\"user_agent\"),\n",
        "                    \"extra\": row.get(\"extra\"),\n",
        "                }\n",
        "\n",
        "    # optional trim per original\n",
        "    if top_ips_per_original is not None:\n",
        "        for _, bucket in store.items():\n",
        "            for _, agg in bucket.items():\n",
        "                keep_ips = set(ip for ip, _ in agg.ip_cnt.most_common(top_ips_per_original*4))\n",
        "                agg.ip_cnt = Counter({ip: cnt for ip, cnt in agg.ip_cnt.items() if ip in keep_ips})\n",
        "                agg.ip_sample = {ip: agg.ip_sample[ip] for ip in list(agg.ip_sample.keys()) if ip in keep_ips}\n",
        "\n",
        "    return masked_counts, store\n",
        "\n",
        "# ---------- SQLite ----------\n",
        "def init_db(db_path):\n",
        "    os.makedirs(os.path.dirname(db_path) or \".\", exist_ok=True)\n",
        "    conn = sqlite3.connect(db_path)\n",
        "    cur = conn.cursor()\n",
        "    cur.execute(\"PRAGMA journal_mode=WAL;\")\n",
        "    cur.execute(\"PRAGMA synchronous=NORMAL;\")\n",
        "    cur.executescript(\"\"\"\n",
        "    DROP TABLE IF EXISTS clusters;\n",
        "    DROP TABLE IF EXISTS masked;\n",
        "    DROP TABLE IF EXISTS originals;\n",
        "    DROP TABLE IF EXISTS original_ips;\n",
        "\n",
        "    CREATE TABLE clusters(\n",
        "      cluster_id INTEGER PRIMARY KEY\n",
        "    );\n",
        "\n",
        "    CREATE TABLE masked(\n",
        "      masked TEXT PRIMARY KEY,\n",
        "      cluster_id INTEGER,\n",
        "      masked_total INTEGER,\n",
        "      FOREIGN KEY(cluster_id) REFERENCES clusters(cluster_id)\n",
        "    );\n",
        "\n",
        "    CREATE TABLE originals(\n",
        "      masked TEXT,\n",
        "      original_url TEXT,\n",
        "      total_cnt INTEGER,\n",
        "      PRIMARY KEY(masked, original_url),\n",
        "      FOREIGN KEY(masked) REFERENCES masked(masked)\n",
        "    );\n",
        "\n",
        "    CREATE TABLE original_ips(\n",
        "      masked TEXT,\n",
        "      original_url TEXT,\n",
        "      ip TEXT,\n",
        "      cnt INTEGER,\n",
        "      sample_line TEXT,\n",
        "      PRIMARY KEY(masked, original_url, ip),\n",
        "      FOREIGN KEY(masked, original_url) REFERENCES originals(masked, original_url)\n",
        "    );\n",
        "\n",
        "    CREATE INDEX idx_masked_cluster ON masked(cluster_id);\n",
        "    CREATE INDEX idx_originals_masked ON originals(masked);\n",
        "    CREATE INDEX idx_origips_masked ON original_ips(masked);\n",
        "    \"\"\")\n",
        "    conn.commit()\n",
        "    return conn\n",
        "\n",
        "def save_to_sqlite(db_path, masked_to_cluster, masked_counts, store,\n",
        "                   top_per_masked=20, top_ips_per_original=3):\n",
        "    conn = init_db(db_path)\n",
        "    cur = conn.cursor()\n",
        "\n",
        "    clusters = sorted(set(masked_to_cluster.values()), key=lambda x:(x==-1, x))\n",
        "    cur.executemany(\"INSERT INTO clusters(cluster_id) VALUES(?)\", [(c,) for c in clusters])\n",
        "\n",
        "    cur.executemany(\n",
        "        \"INSERT INTO masked(masked, cluster_id, masked_total) VALUES(?,?,?)\",\n",
        "        [(m, masked_to_cluster[m], int(masked_counts.get(m, 0))) for m in masked_to_cluster]\n",
        "    )\n",
        "\n",
        "    batch_orig, batch_ip = [], []\n",
        "    for m, bucket in store.items():\n",
        "        for orig, agg in sorted(bucket.items(), key=lambda kv: kv[1].total, reverse=True)[:top_per_masked]:\n",
        "            batch_orig.append((m, orig, int(agg.total)))\n",
        "            for ip, cnt in agg.ip_cnt.most_common(top_ips_per_original):\n",
        "                sample = format_log_line(agg.ip_sample.get(ip))\n",
        "                batch_ip.append((m, orig, ip, int(cnt), sample))\n",
        "            if len(batch_orig) >= 50_000:\n",
        "                cur.executemany(\"INSERT OR REPLACE INTO originals(masked, original_url, total_cnt) VALUES(?,?,?)\", batch_orig)\n",
        "                batch_orig.clear()\n",
        "            if len(batch_ip) >= 50_000:\n",
        "                cur.executemany(\"INSERT OR REPLACE INTO original_ips(masked, original_url, ip, cnt, sample_line) VALUES(?,?,?,?,?)\", batch_ip)\n",
        "                batch_ip.clear()\n",
        "\n",
        "    if batch_orig:\n",
        "        cur.executemany(\"INSERT OR REPLACE INTO originals(masked, original_url, total_cnt) VALUES(?,?,?)\", batch_orig)\n",
        "    if batch_ip:\n",
        "        cur.executemany(\"INSERT OR REPLACE INTO original_ips(masked, original_url, ip, cnt, sample_line) VALUES(?,?,?,?,?)\", batch_ip)\n",
        "\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "def _q(conn, sql, args=()):\n",
        "    cur = conn.execute(sql, args)\n",
        "    cols = [c[0] for c in cur.description]\n",
        "    for row in cur:\n",
        "        yield dict(zip(cols, row))\n",
        "\n",
        "BASE_CSS = \"\"\"\n",
        "body{font-family:Arial, sans-serif; margin:24px;}\n",
        "details{border:1px solid #ddd; border-radius:10px; padding:10px 12px; margin:12px 0; background:#fff;}\n",
        "summary{cursor:pointer; font-weight:bold; outline:none}\n",
        ".badge{display:inline-block; padding:2px 8px; border-radius:12px; background:#eef; margin-left:8px; color:#334}\n",
        "table{border-collapse:collapse; width:100%; margin:8px 0 14px}\n",
        "th,td{border:1px solid #eee; padding:6px; text-align:left}\n",
        "th{background:#fafafa}\n",
        ".small{color:#666; font-size:12px}\n",
        "pre{margin:0; font-family:ui-monospace, SFMono-Regular, Menlo, Consolas, \"Liberation Mono\", monospace; font-size:12px; white-space:pre-wrap}\n",
        "\"\"\"\n",
        "\n",
        "def write_text(path, text):\n",
        "    os.makedirs(os.path.dirname(path) or \".\", exist_ok=True)\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(text)\n",
        "\n",
        "# ---------- Single-page COLLAPSIBLE (tanpa search & sorting) ----------\n",
        "def generate_collapsible_single_html(\n",
        "    db_path,\n",
        "    out_html,\n",
        "    top_per_masked=20,\n",
        "    max_masked_per_cluster=200,\n",
        "    top_ips_per_original=3,\n",
        "):\n",
        "    \"\"\"\n",
        "    Single-page HTML:\n",
        "      - Cluster ‚Üí <details>\n",
        "      - Di dalam cluster: render hingga N masked (MUNCUL SEKALI per masked)\n",
        "      - Untuk tiap masked:\n",
        "          * tampilkan ringkasan (masked, total hits)\n",
        "          * tabel Original URL (top-K)\n",
        "          * untuk setiap original URL: tabel IP (top-M IP) dengan sample-line per IP\n",
        "    \"\"\"\n",
        "    conn = sqlite3.connect(db_path)\n",
        "    clusters = list(_q(conn, \"\"\"\n",
        "        SELECT cluster_id, COUNT(*) AS n_masked, SUM(masked_total) AS hits\n",
        "        FROM masked GROUP BY cluster_id ORDER BY (cluster_id=-1), cluster_id\n",
        "    \"\"\"))\n",
        "\n",
        "    blocks = []\n",
        "    total_masked_rendered = 0\n",
        "\n",
        "    for c in clusters:\n",
        "        cid = c[\"cluster_id\"]\n",
        "        title = \"Noise (-1)\" if cid == -1 else f\"Cluster {cid}\"\n",
        "        n_masked = c[\"n_masked\"]\n",
        "        hits = c[\"hits\"] or 0\n",
        "\n",
        "        # ambil masked yang terbesar di cluster ini\n",
        "        masked_rows = list(_q(conn, \"\"\"\n",
        "            SELECT masked, masked_total\n",
        "            FROM masked\n",
        "            WHERE cluster_id=?\n",
        "            ORDER BY masked_total DESC, masked\n",
        "            LIMIT ?\n",
        "        \"\"\", (cid, max_masked_per_cluster)))\n",
        "\n",
        "        masked_blocks = []\n",
        "        for row in masked_rows:\n",
        "            m = row[\"masked\"]\n",
        "            masked_total = row[\"masked_total\"] or 0\n",
        "\n",
        "            # top-N original URL untuk masked ini\n",
        "            originals = list(_q(conn, \"\"\"\n",
        "                SELECT original_url, total_cnt\n",
        "                FROM originals\n",
        "                WHERE masked=?\n",
        "                ORDER BY total_cnt DESC\n",
        "                LIMIT ?\n",
        "            \"\"\", (m, top_per_masked)))\n",
        "\n",
        "            # bangun tabel original URL\n",
        "            orig_rows_html = []\n",
        "            for orig in originals:\n",
        "                o_url = orig[\"original_url\"]\n",
        "                o_total = orig[\"total_cnt\"] or 0\n",
        "\n",
        "                # top-M IP untuk original ini\n",
        "                ip_rows = list(_q(conn, \"\"\"\n",
        "                    SELECT ip, cnt, sample_line\n",
        "                    FROM original_ips\n",
        "                    WHERE masked=? AND original_url=?\n",
        "                    ORDER BY cnt DESC\n",
        "                    LIMIT ?\n",
        "                \"\"\", (m, o_url, top_ips_per_original)))\n",
        "\n",
        "                # tabel IP (di-bungkus agar rapi)\n",
        "                ip_tbl_rows = []\n",
        "                for ipr in ip_rows:\n",
        "                    share = (ipr[\"cnt\"] / masked_total) if masked_total else 0.0\n",
        "                    ip_tbl_rows.append(\n",
        "                        \"<tr>\"\n",
        "                        f\"<td>{escape(ipr['ip'] or '-')}</td>\"\n",
        "                        f\"<td><pre>{escape(ipr['sample_line'] or '')}</pre></td>\"\n",
        "                        f\"<td>{ipr['cnt']}</td>\"\n",
        "                        f\"<td>{share:.2%}</td>\"\n",
        "                        \"</tr>\"\n",
        "                    )\n",
        "\n",
        "                ip_table_html = (\n",
        "                    \"<table>\"\n",
        "                    \"<thead><tr><th>IP</th><th>Sample Log Line (first seen per IP)</th><th>Count</th><th>Share</th></tr></thead>\"\n",
        "                    f\"<tbody>{''.join(ip_tbl_rows)}</tbody>\"\n",
        "                    \"</table>\"\n",
        "                )\n",
        "\n",
        "                # baris untuk original URL (URL dan totalnya), lalu tabel IP di bawahnya\n",
        "                orig_rows_html.append(\n",
        "                    \"<tr>\"\n",
        "                    f\"<td style='width:40%;word-break:break-all'>{escape(o_url)}</td>\"\n",
        "                    f\"<td style='width:10%'>{o_total}</td>\"\n",
        "                    f\"<td>{ip_table_html}</td>\"\n",
        "                    \"</tr>\"\n",
        "                )\n",
        "\n",
        "            if not orig_rows_html:\n",
        "                # kalau tidak ada original (jarang terjadi), skip blok masked\n",
        "                continue\n",
        "\n",
        "            # satu blok <details> PER MASKED (muncul sekali)\n",
        "            masked_blocks.append(f\"\"\"\n",
        "            <details>\n",
        "              <summary><code>{escape(m)}</code>\n",
        "                <span class=\"badge\">masked hits: {masked_total}</span>\n",
        "                <span class=\"badge\">showing top {top_per_masked} originals √ó top {top_ips_per_original} IP</span>\n",
        "              </summary>\n",
        "              <table>\n",
        "                <thead>\n",
        "                  <tr><th>Original URL</th><th>Total Hits (URL)</th><th>Per-IP Samples</th></tr>\n",
        "                </thead>\n",
        "                <tbody>\n",
        "                  {''.join(orig_rows_html)}\n",
        "                </tbody>\n",
        "              </table>\n",
        "            </details>\n",
        "            \"\"\")\n",
        "\n",
        "        cluster_html = f\"\"\"\n",
        "        <div class=\"cluster-container\">\n",
        "          <details>\n",
        "            <summary>{escape(title)}\n",
        "              <span class=\"badge\">masked: {n_masked}</span>\n",
        "              <span class=\"badge\">hits: {hits}</span>\n",
        "              <span class=\"small\">render up to {min(max_masked_per_cluster, n_masked)} masked</span>\n",
        "            </summary>\n",
        "            {''.join(masked_blocks)}\n",
        "          </details>\n",
        "        </div>\n",
        "        \"\"\"\n",
        "        blocks.append(cluster_html)\n",
        "        total_masked_rendered += len(masked_rows)\n",
        "\n",
        "    conn.close()\n",
        "\n",
        "    html = f\"\"\"<!doctype html>\n",
        "<html><head><meta charset=\"utf-8\">\n",
        "<title>Cluster Report (Cluster ‚Üí Masked ‚Üí Original ‚Üí IP)</title>\n",
        "<style>\n",
        "{BASE_CSS}\n",
        "/* kecilkan pre agar muat */\n",
        "pre{{margin:0; font-size:12px; white-space:pre-wrap}}\n",
        "code{{background:#f6f8fa; padding:2px 6px; border-radius:6px}}\n",
        "summary .badge{{margin-left:6px}}\n",
        "table td{{vertical-align:top}}\n",
        "</style>\n",
        "</head><body>\n",
        "<h2>Cluster Report ‚Äî Grouped by Masked (no search/sort)</h2>\n",
        "<div class=\"small\">Rendered masked total (max per cluster): {total_masked_rendered}</div>\n",
        "{\"\".join(blocks)}\n",
        "</body></html>\"\"\"\n",
        "    write_text(out_html, html)\n",
        "\n",
        "\n",
        "# ---------- (opsional) Multi-page sederhana (tanpa search/sort) ----------\n",
        "def generate_site_from_sqlite(db_path, outdir, page_size=50, top_per_masked=20, top_ips_per_original=3):\n",
        "    conn = sqlite3.connect(db_path)\n",
        "\n",
        "    clusters = list(_q(conn, \"SELECT cluster_id, COUNT(*) AS n_masked, SUM(masked_total) AS hits FROM masked GROUP BY cluster_id ORDER BY (cluster_id=-1), cluster_id\"))\n",
        "    rows = \"\".join(\n",
        "        f\"<tr><td><a href='cluster_{c['cluster_id']}_1.html'>{'Noise (-1)' if c['cluster_id']==-1 else 'Cluster '+str(c['cluster_id'])}</a></td>\"\n",
        "        f\"<td>{c['n_masked']}</td><td>{c['hits'] or 0}</td></tr>\"\n",
        "        for c in clusters\n",
        "    )\n",
        "    idx_html = f\"\"\"<!doctype html><html><head><meta charset='utf-8'><title>Clusters Index</title>\n",
        "<style>{BASE_CSS}</style></head><body>\n",
        "<h2>Clusters Index</h2>\n",
        "<table><thead><tr><th>Cluster</th><th># Masked</th><th>Total Hits</th></tr></thead><tbody>\n",
        "{rows}\n",
        "</tbody></table>\n",
        "</body></html>\"\"\"\n",
        "    write_text(os.path.join(outdir, \"index.html\"), idx_html)\n",
        "\n",
        "    cluster_ids = [c[\"cluster_id\"] for c in clusters]\n",
        "    for cid in cluster_ids:\n",
        "        total = next(_q(conn, \"SELECT COUNT(*) AS n FROM masked WHERE cluster_id=?\", (cid,)))[\"n\"]\n",
        "        pages = max(1, math.ceil(total / page_size))\n",
        "        for page in range(1, pages+1):\n",
        "            offset = (page-1)*page_size\n",
        "            masked_rows = list(_q(conn,\n",
        "                \"SELECT masked, masked_total FROM masked WHERE cluster_id=? ORDER BY masked_total DESC, masked LIMIT ? OFFSET ?\",\n",
        "                (cid, page_size, offset)\n",
        "            ))\n",
        "            title = \"Noise (-1)\" if cid == -1 else f\"Cluster {cid}\"\n",
        "\n",
        "            blocks = []\n",
        "            for row in masked_rows:\n",
        "                m = row[\"masked\"]\n",
        "                masked_total = row[\"masked_total\"] or 0\n",
        "                originals = list(_q(conn,\n",
        "                    \"SELECT original_url, total_cnt FROM originals WHERE masked=? ORDER BY total_cnt DESC LIMIT ?\",\n",
        "                    (m, top_per_masked)\n",
        "                ))\n",
        "                orows = \"\"\n",
        "                for orig in originals:\n",
        "                    ip_rows = list(_q(conn,\n",
        "                        \"SELECT ip, cnt, sample_line FROM original_ips WHERE masked=? AND original_url=? ORDER BY cnt DESC LIMIT ?\",\n",
        "                        (m, orig[\"original_url\"], top_ips_per_original)\n",
        "                    ))\n",
        "                    for ipr in ip_rows:\n",
        "                        share = (ipr[\"cnt\"] / masked_total) if masked_total else 0.0\n",
        "                        orows += f\"<tr><td>{escape(ipr['ip'] or '-')}</td><td><pre>{escape(ipr['sample_line'] or '')}</pre></td><td>{ipr['cnt']}</td><td>{share:.2%}</td></tr>\"\n",
        "\n",
        "                blocks.append(f\"\"\"\n",
        "                <div class='cluster-container'>\n",
        "                  <div><strong>{escape(m)}</strong>\n",
        "                    <span class='badge'>hits: {masked_total}</span>\n",
        "                  </div>\n",
        "                  <table><thead><tr><th>IP</th><th>Sample Log Line</th><th>Count</th><th>Share</th></tr></thead>\n",
        "                  <tbody>{orows}</tbody></table>\n",
        "                </div>\n",
        "                \"\"\")\n",
        "\n",
        "            pager = \" \".join(\n",
        "                (f\"<strong>{p}</strong>\" if p==page else f\"<a href='cluster_{cid}_{p}.html'>{p}</a>\")\n",
        "                for p in range(1, pages+1)\n",
        "            )\n",
        "            html = f\"\"\"<!doctype html><html><head><meta charset='utf-8'><title>{title}</title>\n",
        "<style>{BASE_CSS}</style></head><body>\n",
        "<p><a href='index.html'>&larr; Back to index</a></p>\n",
        "<h2>{title}</h2>\n",
        "<div class='pager'>Pages: {pager}</div>\n",
        "{''.join(blocks)}\n",
        "<div class='pager'>Pages: {pager}</div>\n",
        "</body></html>\"\"\"\n",
        "            os.makedirs(outdir, exist_ok=True)\n",
        "            write_text(os.path.join(outdir, f\"cluster_{cid}_{page}.html\"), html)\n",
        "\n",
        "    conn.close()\n",
        "\n",
        "def main():\n",
        "    ap = argparse.ArgumentParser(description=\"Expand masked clusters ‚Üí per-original per-IP sample lines (SQLite, no search/sort).\")\n",
        "    ap.add_argument(\"decoded_csv\", help=\"outputs/access_decoded.csv (hasil decoder)\")\n",
        "    ap.add_argument(\"cluster_csv\", help=\"outputs/access_optics.csv (masked,cluster)\")\n",
        "    ap.add_argument(\"--top-per-masked\", type=int, default=20, help=\"Top-N original URL per masked\")\n",
        "    ap.add_argument(\"--top-ips-per-original\", type=int, default=3, help=\"Top-M IP per original URL\")\n",
        "    ap.add_argument(\"--max-originals-per-masked\", type=int, default=200, help=\"Batas variasi original per masked\")\n",
        "\n",
        "    # SQLite + HTML\n",
        "    ap.add_argument(\"--db\", default=\"outputs/cluster_report.db\", help=\"SQLite DB output\")\n",
        "    ap.add_argument(\"--single-html\", default=\"outputs/expanded_collapsible.html\", help=\"Single-page HTML (collapsible, no search/sort)\")\n",
        "    ap.add_argument(\"--max-masked-per-cluster\", type=int, default=200, help=\"Batas masked dirender per cluster\")\n",
        "\n",
        "    # (opsional) Multi-page\n",
        "    ap.add_argument(\"--site-dir\", default=\"\", help=\"Folder output HTML statis (multi-page, optional)\")\n",
        "    ap.add_argument(\"--page-size\", type=int, default=50, help=\"# masked per halaman (multi-page optional)\")\n",
        "    args = ap.parse_args()\n",
        "\n",
        "    print(\"üîπ Load clustered masked ‚Üí cluster...\")\n",
        "    masked_to_cluster, _ = load_clustered_masked(args.cluster_csv)\n",
        "    print(f\"  masked patterns: {len(masked_to_cluster):,}\")\n",
        "\n",
        "    print(\"üîπ Streaming decoded CSV ‚Üí agregasi per original & per-IP...\")\n",
        "    masked_counts, store = stream_count_originals(\n",
        "        args.decoded_csv,\n",
        "        masked_to_cluster,\n",
        "        max_originals_per_masked=args.max_originals_per_masked,\n",
        "        top_ips_per_original=args.top_ips_per_original,\n",
        "    )\n",
        "\n",
        "    print(f\"üîπ Simpan ke SQLite: {args.db}\")\n",
        "    save_to_sqlite(args.db, masked_to_cluster, masked_counts, store,\n",
        "                   top_per_masked=args.top_per_masked,\n",
        "                   top_ips_per_original=args.top_ips_per_original)\n",
        "\n",
        "    if args.single_html:\n",
        "        print(f\"üîπ Generate single HTML (collapsible) ‚Üí {args.single_html}\")\n",
        "        generate_collapsible_single_html(args.db, args.single_html,\n",
        "                                         top_per_masked=args.top_per_masked,\n",
        "                                         max_masked_per_cluster=args.max_masked_per_cluster,\n",
        "                                         top_ips_per_original=args.top_ips_per_original)\n",
        "\n",
        "    if args.site_dir:\n",
        "        print(f\"üîπ Generate HTML statis (multi-page) ‚Üí {args.site_dir}\")\n",
        "        generate_site_from_sqlite(args.db, args.site_dir,\n",
        "                                  page_size=args.page_size,\n",
        "                                  top_per_masked=args.top_per_masked,\n",
        "                                  top_ips_per_original=args.top_ips_per_original)\n",
        "\n",
        "    print(\"‚úÖ Done.\")\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîπ Load clustered masked ‚Üí cluster...\n",
            "  masked patterns: 92,150\n",
            "üîπ Streaming decoded CSV ‚Üí agregasi per original & per-IP...\n",
            "üîπ Simpan ke SQLite: outputs\\cluster_report.db\n",
            "üîπ Generate single HTML (collapsible) ‚Üí outputs\\expanded_collapsible.html\n",
            "‚úÖ Done.\n"
          ]
        }
      ],
      "source": [
        "! .\\.venv\\Scripts\\python -m app.expand_originals outputs\\access_decoded.csv outputs\\access_optics.csv --db outputs\\cluster_report.db --single-html outputs\\expanded_collapsible.html --top-per-masked 20 --top-ips-per-original 3 --max-originals-per-masked 200 --max-masked-per-cluster 200"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

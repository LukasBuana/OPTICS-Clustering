{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7ghYEg0J14k",
        "outputId": "b00815fe-db8d-4aee-d5f1-7cde08cc60be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing requirements.txt\n"
          ]
        }
      ],
      "source": [
        "%%writefile requirements.txt\n",
        "torch==2.7.1\n",
        "transformers==4.44.2\n",
        "scikit-learn==1.6.1\n",
        "pandas==2.3.0\n",
        "numpy==2.2.6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "t1C1XpB1Oqf4"
      },
      "outputs": [],
      "source": [
        "# !pip install -r requirements.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LWYJEJpaL6Mo"
      },
      "outputs": [],
      "source": [
        "!mkdir app inputs outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTQWxoMDLxwf",
        "outputId": "22e06ab3-0e7d-499d-f638-e451a64d2cab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing app/bot.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app/bot.py\n",
        "import pickle\n",
        "import socket\n",
        "import atexit\n",
        "\n",
        "# Known bot suffixes used for reverse DNS validation\n",
        "KNOWN_BOTS = {\n",
        "    \"Googlebot\": [\".googlebot.com\"],\n",
        "    \"Bingbot\": [\".search.msn.com\"],\n",
        "    \"AhrefsBot\": [\".ahrefs.com\", \".ahrefs.net\"],\n",
        "    \"YandexBot\": [\".yandex.ru\", \".yandex.com\", \".yandex.net\"],\n",
        "    \"SemrushBot\": [\".semrush.com\"],\n",
        "    \"DuckDuckBot\": [\".duckduckgo.com\"],\n",
        "    \"MJ12bot\": [\".majestic12.co.uk\"],\n",
        "    \"Slurp\": [\".crawl.yahoo.net\"],\n",
        "    \"Applebot\": [\".apple.com\"]\n",
        "}\n",
        "\n",
        "# Cache file paths for verified and spoofed bot IPs\n",
        "VERIFIED_IP_FILE = \"verified_bots.pkl\"\n",
        "SPOOFED_IP_FILE = \"spoofed_bots.pkl\"\n",
        "\n",
        "def pickle_load(path):\n",
        "    \"\"\"\n",
        "    Load a Python set object from a pickle file.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(path, \"rb\") as f:\n",
        "            return pickle.load(f)\n",
        "    except Exception:\n",
        "        return set()\n",
        "\n",
        "# Load IP caches early to avoid NameError during runtime\n",
        "verified_bot_ips = pickle_load(VERIFIED_IP_FILE)\n",
        "spoofed_bot_ips = pickle_load(SPOOFED_IP_FILE)\n",
        "\n",
        "def save_ip_caches():\n",
        "    \"\"\"\n",
        "    Persist the sets of verified and spoofed bot IP addresses to disk.\n",
        "    \"\"\"\n",
        "    with open(VERIFIED_IP_FILE, \"wb\") as f:\n",
        "        pickle.dump(verified_bot_ips, f)\n",
        "    with open(SPOOFED_IP_FILE, \"wb\") as f:\n",
        "        pickle.dump(spoofed_bot_ips, f)\n",
        "\n",
        "atexit.register(save_ip_caches)\n",
        "\n",
        "def reverse_dns(ip):\n",
        "    \"\"\"\n",
        "    Perform a reverse DNS lookup for a given IP address.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        return socket.gethostbyaddr(ip)[0]\n",
        "    except socket.herror:\n",
        "        return None\n",
        "\n",
        "def forward_dns(hostname):\n",
        "    \"\"\"\n",
        "    Perform a forward DNS lookup for a given hostname.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        return socket.gethostbyname(hostname)\n",
        "    except socket.gaierror:\n",
        "        return None\n",
        "\n",
        "def is_valid_bot(ip, ua):\n",
        "    \"\"\"\n",
        "    Determine if a request is from a legitimate search engine bot.\n",
        "    \"\"\"\n",
        "    if ip in verified_bot_ips:\n",
        "        return True\n",
        "    if ip in spoofed_bot_ips:\n",
        "        return False\n",
        "\n",
        "    for bot, suffixes in KNOWN_BOTS.items():\n",
        "        if bot.lower() in ua.lower():\n",
        "            rdns = reverse_dns(ip)\n",
        "            if not rdns or not any(rdns.endswith(sfx) for sfx in suffixes):\n",
        "                spoofed_bot_ips.add(ip)\n",
        "                return False\n",
        "            if forward_dns(rdns) != ip:\n",
        "                spoofed_bot_ips.add(ip)\n",
        "                return False\n",
        "            verified_bot_ips.add(ip)\n",
        "            return True\n",
        "\n",
        "    return False  # Not a known bot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sR9LTjwtMN_y",
        "outputId": "9167160c-3464-4820-e392-634f455d44fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting app/decoder.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app/decoder.py\n",
        "import os\n",
        "import re\n",
        "import sys\n",
        "import codecs\n",
        "import csv  # <<< Added for streaming\n",
        "import argparse\n",
        "import urllib.parse\n",
        "import base64\n",
        "# import pandas as pd # <<< Removed pandas dependency for streaming CSV\n",
        "from app.bot import is_valid_bot\n",
        "\n",
        "# Variables\n",
        "log_pattern = re.compile(\n",
        "    r'(?P<ip>\\S+) - - \\[(?P<time>[^\\]]+)\\] '\n",
        "    r'\"(?P<method>\\S+) (?P<url>\\S+) (?P<protocol>[^\"]+)\" '\n",
        "    r'(?P<status>\\d+) (?P<size>\\d+) '\n",
        "    r'\"(?P<referrer>[^\"]*)\" \"(?P<user_agent>[^\"]*)\" \"(?P<extra>[^\"]*)\"'\n",
        ")\n",
        "\n",
        "# Functions\n",
        "def esc_nl(text):\n",
        "    \"\"\"\n",
        "    Escape newline and carriage return characters in a string.\n",
        "    \"\"\"\n",
        "    return text.replace('\\n', '\\\\n').replace('\\r', '\\\\r').strip()\n",
        "\n",
        "\n",
        "def dec_url(text):\n",
        "    \"\"\"\n",
        "    Decode a URL-encoded string up to two iterations.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        first = urllib.parse.unquote(text)\n",
        "        if first == text:\n",
        "            return text\n",
        "\n",
        "        second = urllib.parse.unquote(first)\n",
        "        if second == first:\n",
        "            return first\n",
        "\n",
        "        return second\n",
        "    except Exception:\n",
        "        return text\n",
        "\n",
        "\n",
        "def dec_esc(text):\n",
        "    \"\"\"\n",
        "    Decode escaped character sequences such as \\\\xNN and \\\\uNNNN.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if '\\\\x' in text or '\\\\u' in text:\n",
        "            decoded = codecs.escape_decode(text.encode())[0].decode('utf-8', errors='replace')\n",
        "            return decoded\n",
        "        return text\n",
        "    except Exception:\n",
        "        return text\n",
        "\n",
        "\n",
        "def dec_base64(text):\n",
        "    \"\"\"\n",
        "    Detect and decode a Base64-encoded segment in the final URL path.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        last_part = text.rsplit(\"/\", 1)[-1]\n",
        "\n",
        "        # Heuristic: reasonably long, valid base64 characters\n",
        "        if re.fullmatch(r'[A-Za-z0-9+/=]{8,}', last_part):\n",
        "            decoded = base64.b64decode(last_part, validate=False).decode('utf-8', errors='ignore')\n",
        "            annotated = f\"{text}(base64:{decoded})\"\n",
        "            return annotated\n",
        "\n",
        "        return text\n",
        "    except Exception:\n",
        "        return text\n",
        "\n",
        "\n",
        "def dec_combined(text):\n",
        "    \"\"\"\n",
        "    Apply a sequence of decoding techniques to a string:\n",
        "    1. URL decoding (up to two iterations)\n",
        "    2. Escape sequence decoding (e.g., \\\\xNN, \\\\uNNNN)\n",
        "    3. Base64 decoding on the last URL path segment\n",
        "    \"\"\"\n",
        "    text = dec_url(text)\n",
        "    text = dec_esc(text)\n",
        "    text = dec_base64(text)\n",
        "    return text\n",
        "\n",
        "\n",
        "def parse_dec_line(line):\n",
        "    \"\"\"\n",
        "    Parse and decode a single NGINX access log line.\n",
        "    \"\"\"\n",
        "    match = log_pattern.match(line)\n",
        "    if not match:\n",
        "        return None, None  # Unparsable log line\n",
        "\n",
        "    fields = match.groupdict()\n",
        "\n",
        "    # Decode URL field (multi-step decoding)\n",
        "    fields['url'] = dec_combined(fields['url'])\n",
        "\n",
        "    # Decode referrer field (only take the decoded text, not flags)\n",
        "    fields['referrer'] = dec_combined(fields['referrer'])\n",
        "\n",
        "    # Apply newline escaping cleanup\n",
        "    for key in fields:\n",
        "        fields[key] = esc_nl(fields[key])\n",
        "\n",
        "    decoded = (\n",
        "        f'{fields[\"ip\"]} - - [{fields[\"time\"]}] '\n",
        "        f'\"{fields[\"method\"]} {fields[\"url\"]} {fields[\"protocol\"]}\" '\n",
        "        f'{fields[\"status\"]} {fields[\"size\"]} '\n",
        "        f'\"{fields[\"referrer\"]}\" \"{fields[\"user_agent\"]}\" \"{fields[\"extra\"]}\"'\n",
        "    )\n",
        "\n",
        "    return decoded, fields\n",
        "\n",
        "\n",
        "def parse_dec_file(in_path, out_path):\n",
        "    \"\"\"\n",
        "    Decode and clean all entries in a log file and write the results to a new file.\n",
        "    \"\"\"\n",
        "    with open(in_path, 'r', encoding='utf-8', errors='replace') as in_file, \\\n",
        "        open(out_path, 'w', encoding='utf-8') as out_file:\n",
        "\n",
        "        for line in in_file:\n",
        "            decoded, fields = parse_dec_line(line)\n",
        "\n",
        "            if not fields:\n",
        "                continue  # Skip unparsed line\n",
        "\n",
        "            if is_valid_bot(fields['ip'], fields['user_agent']):\n",
        "                continue  # Skip valid bot\n",
        "\n",
        "            out_file.write(f\"{decoded}\\n\")\n",
        "\n",
        "\n",
        "# Hapus parse_dec_file_to_dataframe dan parse_dec_file_to_csv (lama)\n",
        "# Ganti dengan fungsi streaming yang baru\n",
        "def parse_dec_file_stream_to_csv(in_path, out_path):\n",
        "    \"\"\"\n",
        "    Decode and clean all entries in a log file and stream them to a CSV file.\n",
        "    This avoids loading all data into RAM (MemoryError).\n",
        "    \"\"\"\n",
        "    print(\"‚è≥ Starting streaming decode to CSV...\")\n",
        "    \n",
        "    with open(in_path, 'r', encoding='utf-8', errors='replace') as in_file, \\\n",
        "         open(out_path, 'w', encoding='utf-8', newline='') as out_file:\n",
        "\n",
        "        writer = None\n",
        "        \n",
        "        for no, line in enumerate(in_file, 1):\n",
        "            _, fields = parse_dec_line(line)\n",
        "\n",
        "            if not fields:\n",
        "                continue  # Skip unparsed line\n",
        "\n",
        "            if is_valid_bot(fields['ip'], fields['user_agent']):\n",
        "                continue  # Skip valid bot\n",
        "\n",
        "            # Add line number (optional, but in the original code)\n",
        "            fields['no'] = no\n",
        "            \n",
        "            # Initialize CSV writer after the first record is processed (to get fieldnames)\n",
        "            if writer is None:\n",
        "                fieldnames = list(fields.keys())\n",
        "                writer = csv.DictWriter(out_file, fieldnames=fieldnames)\n",
        "                writer.writeheader()\n",
        "                \n",
        "            writer.writerow(fields)\n",
        "            \n",
        "            # Progress update (optional)\n",
        "            if no % 100000 == 0:\n",
        "                print(f\"  Processed {no:,} lines...\")\n",
        "\n",
        "    print(\"‚úÖ Streaming decode complete.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser(description=\"NGINX log decoder.\")\n",
        "    parser.add_argument(\"in_file\", help=\"NGINX log file\")\n",
        "    parser.add_argument(\"out_file\", help=\"The decoded NGINX log file\")\n",
        "    parser.add_argument(\"--csv\", action=\"store_true\", help=\"Save the output in CSV format.\")\n",
        "\n",
        "    # Parse the arguments\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    if not os.path.exists(args.in_file):\n",
        "        print(f\"‚ùå File not found: '{args.in_file}'\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    out_dir = os.path.dirname(args.out_file)\n",
        "    if out_dir and not os.path.exists(out_dir):\n",
        "        # NOTE: Instead of checking existence, better to create it if it doesn't exist\n",
        "        try:\n",
        "            os.makedirs(out_dir, exist_ok=True)\n",
        "        except OSError as e:\n",
        "            print(f\"‚ùå Cannot create output directory: '{out_dir}' - {e}\")\n",
        "            sys.exit(1)\n",
        "        \n",
        "    if args.csv:\n",
        "        # Use the memory-efficient streaming function\n",
        "        parse_dec_file_stream_to_csv(args.in_file, args.out_file)\n",
        "    else:\n",
        "        parse_dec_file(args.in_file, args.out_file)\n",
        "\n",
        "    print(f\"‚úÖ Log file successfully converted to {args.out_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uivwcyzrMRRU",
        "outputId": "a3fc95c3-a5d2-48ff-fd01-6071b13882f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting app/main.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app/main.py\n",
        "# app/main.py\n",
        "import os\n",
        "import re\n",
        "import csv\n",
        "import sys\n",
        "import argparse\n",
        "import warnings\n",
        "import time \n",
        "from collections import Counter\n",
        "from urllib.parse import urlparse, unquote\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import BertTokenizerFast, BertModel\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.metrics import pairwise_distances, silhouette_score, davies_bouldin_score\n",
        "\n",
        "# =========================\n",
        "# IMPORT KHUSUS GPU (CUML/CUPY) DAN FALLBACK KE SKLEARN (CPU)\n",
        "# =========================\n",
        "try:\n",
        "    # Coba import CuML dan CuPy\n",
        "    from cuml.cluster import OPTICS as CumlOPTICS\n",
        "    import cupy as cp\n",
        "    \n",
        "    # Periksa apakah CuPy benar-benar bisa menggunakan GPU\n",
        "    if cp.cuda.is_available():\n",
        "        HAS_GPU_CLUSTERING = True\n",
        "        OPTICS_IMPL = CumlOPTICS\n",
        "    else:\n",
        "        # Paksakan fallback jika CUDA tidak tersedia di CuPy\n",
        "        raise RuntimeError(\"CuPy terinstal tetapi GPU tidak terdeteksi atau siap.\")\n",
        "\n",
        "# Tangkap ImportError, RuntimeError, atau Exception lain\n",
        "except (ImportError, RuntimeError, Exception) as e: \n",
        "    # Fallback ke Scikit-learn OPTICS\n",
        "    #print(f\"DEBUG: CuML/CuPy import atau inisialisasi gagal: {e}\") # Buka jika perlu debugging\n",
        "    from sklearn.cluster import OPTICS as SklearnOPTICS\n",
        "    OPTICS_IMPL = SklearnOPTICS\n",
        "    cp = np \n",
        "    HAS_GPU_CLUSTERING = False\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Konfigurasi & Inisialisasi\n",
        "# =========================\n",
        "\n",
        "# Sembunyikan FutureWarning transformers yang tidak fatal\n",
        "warnings.filterwarnings(\n",
        "    \"ignore\",\n",
        "    message=\"`clean_up_tokenization_spaces` was not set\",\n",
        "    category=FutureWarning,\n",
        ")\n",
        "\n",
        "# Pilih device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "if HAS_GPU_CLUSTERING and device.type == \"cuda\":\n",
        "    print(\"‚úÖ GPU Acceleration aktif untuk BERT dan OPTICS (via CuML).\")\n",
        "elif HAS_GPU_CLUSTERING and device.type == \"cpu\":\n",
        "    print(\"‚ö†Ô∏è Peringatan: CuML terinstal, tetapi PyTorch menggunakan CPU. Klastering GPU tidak efektif.\")\n",
        "else:\n",
        "    print(\"‚ùå GPU Clustering (CuML) tidak terinstal. Menggunakan Scikit-learn (CPU).\")\n",
        "\n",
        "\n",
        "# Muat tokenizer & model BERT\n",
        "TOKENIZER = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
        "torch_dtype = torch.float16 if device.type == \"cuda\" else torch.float32\n",
        "MODEL = BertModel.from_pretrained(\"bert-base-uncased\", torch_dtype=torch_dtype).to(device)\n",
        "MODEL.eval()\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Utilitas parsing & masking (Tidak Berubah)\n",
        "# =========================\n",
        "\n",
        "def split_url_tokens(url: str):\n",
        "    \"\"\"Pecah URL menjadi token sederhana dari path & query.\"\"\"\n",
        "    parsed = urlparse(url)\n",
        "    path = unquote(parsed.path or \"\")\n",
        "    query = unquote(parsed.query or \"\")\n",
        "    delimiters = r\"[\\/\\-\\_\\=\\&\\?\\.\\+\\(\\)\\[\\]\\<\\>\\{\\}]\"\n",
        "    tokens = re.split(delimiters, path.strip(\"/\")) + re.split(delimiters, query)\n",
        "    return [tok for tok in tokens if tok]\n",
        "\n",
        "\n",
        "def iter_urls_from_decoded_csv(csv_path: str, mask_numbers: bool = True):\n",
        "    \"\"\"Streaming baca kolom 'url' dari CSV hasil decoder (hemat RAM).\"\"\"\n",
        "    with open(csv_path, \"r\", encoding=\"utf-8\", errors=\"ignore\", newline=\"\") as f:\n",
        "        reader = csv.DictReader(f)\n",
        "        if \"url\" not in reader.fieldnames:\n",
        "            raise ValueError(\n",
        "                f\"CSV {csv_path} tidak memiliki kolom 'url'. \"\n",
        "                \"Pastikan file dibuat oleh app.decoder dengan flag --csv.\"\n",
        "            )\n",
        "        for row in reader:\n",
        "            url = row[\"url\"]\n",
        "            if mask_numbers:\n",
        "                url = re.sub(r\"\\d+\", \"<NUM>\", url)\n",
        "            yield url\n",
        "\n",
        "\n",
        "def topk_urls(csv_path: str, k: int = 200_000):\n",
        "    \"\"\"Hitung frekuensi URL (masked) secara streaming, dan ambil Top-K paling sering.\"\"\"\n",
        "    cnt = Counter()\n",
        "    for url in iter_urls_from_decoded_csv(csv_path):\n",
        "        cnt[url] += 1\n",
        "    return [u for u, _ in cnt.most_common(k)]\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Embedding (memmap, per-batch) (Tidak Berubah)\n",
        "# =========================\n",
        "\n",
        "def generate_url_embeddings_to_memmap(\n",
        "    url_list,\n",
        "    memmap_path: str,\n",
        "    batch_size: int = 128,\n",
        "    max_length: int = 64,\n",
        "    verbose: bool = False,\n",
        "):\n",
        "    \"\"\"Tulis embedding BERT ke file memmap (float32) per-batch ‚Äî hemat RAM.\"\"\"\n",
        "    n = len(url_list)\n",
        "    if n == 0:\n",
        "        raise ValueError(\"url_list kosong ‚Äî tidak ada URL untuk di-embed.\")\n",
        "\n",
        "    out_dir = os.path.dirname(memmap_path)\n",
        "    if out_dir:\n",
        "        os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    hidden_size = MODEL.config.hidden_size\n",
        "    embs = np.memmap(memmap_path, dtype=\"float32\", mode=\"w+\", shape=(n, hidden_size))\n",
        "    \n",
        "    start_time = time.time()\n",
        "    i = 0\n",
        "    batch_count = 0\n",
        "    total_batches = (n + batch_size - 1) // batch_size\n",
        "    \n",
        "    while i < n:\n",
        "        j = min(i + batch_size, n)\n",
        "        batch = url_list[i:j]\n",
        "        \n",
        "        batch_count += 1\n",
        "        \n",
        "        inputs = TOKENIZER(\n",
        "            batch,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=max_length,\n",
        "        ).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = MODEL(**inputs)\n",
        "            batch_emb = outputs.last_hidden_state.mean(dim=1)\n",
        "            if batch_emb.dtype != torch.float32:\n",
        "                batch_emb = batch_emb.float()\n",
        "\n",
        "        embs[i:j, :] = batch_emb.detach().cpu().numpy()\n",
        "        i = j\n",
        "\n",
        "        # Progress reporting\n",
        "        if verbose and batch_count % 10 == 0 or i == n:\n",
        "            elapsed = time.time() - start_time\n",
        "            rate = i / elapsed if elapsed > 0 else 0\n",
        "            \n",
        "            remaining_seconds = (n - i) / rate if rate > 0 else 0\n",
        "            \n",
        "            if remaining_seconds >= 3600:\n",
        "                eta_str = f\"{remaining_seconds/3600:.1f}h\"\n",
        "            elif remaining_seconds >= 60:\n",
        "                eta_str = f\"{remaining_seconds/60:.1f}m\"\n",
        "            else:\n",
        "                eta_str = f\"{remaining_seconds:.1f}s\"\n",
        "                \n",
        "            print(\n",
        "                f\"[BATCH {batch_count}/{total_batches}] Embedded: {i}/{n} | Rate: {rate:.1f} url/s | ETA: {eta_str}\",\n",
        "                end='\\r' if i < n else '\\n',\n",
        "                flush=True\n",
        "            )\n",
        "\n",
        "    embs.flush() \n",
        "    del embs\n",
        "    return memmap_path\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Evaluasi Metriks (Memastikan Metrik yang Dipilih Digunakan)\n",
        "# =========================\n",
        "\n",
        "def evaluate_clustering_metrics(embs: np.ndarray, labels: np.ndarray, metric: str = \"cosine\"):\n",
        "    \"\"\"Hitung metrik evaluasi klastering (Silhouette dan DBI).\"\"\"\n",
        "    # Filter hanya data yang terklaster (label != -1)\n",
        "    clustered_indices = labels != -1\n",
        "    X_clustered = embs[clustered_indices]\n",
        "    labels_clustered = labels[clustered_indices]\n",
        "\n",
        "    n_clusters_total = len(np.unique(labels)) \n",
        "    n_outliers = np.sum(labels == -1)\n",
        "    \n",
        "    unique_clustered_labels = np.unique(labels_clustered)\n",
        "    n_clusters_actual = len(unique_clustered_labels)\n",
        "\n",
        "    if n_clusters_actual < 2:\n",
        "        print(f\"‚ö†Ô∏è Hanya {n_clusters_actual} klaster valid ditemukan (di luar outlier).\")\n",
        "        print(\"‚ö†Ô∏è Silhouette dan DBI memerlukan setidaknya 2 klaster.\")\n",
        "        return {\n",
        "            \"N_URLS\": len(labels),\n",
        "            \"N_CLUSTERS_TOTAL\": n_clusters_total,\n",
        "            \"N_CLUSTERS_ACTUAL\": n_clusters_actual,\n",
        "            \"N_OUTLIERS\": int(n_outliers),\n",
        "            \"SILHOUETTE_SCORE\": np.nan,\n",
        "            \"DAVIES_BOULDIN_INDEX\": np.nan,\n",
        "            \"DISTANCE_METRIC\": metric,\n",
        "        }\n",
        "\n",
        "    print(f\"üìä Menghitung Silhouette Score ({metric})...\")\n",
        "    try:\n",
        "        # Silhouette score menggunakan metrik yang dipilih oleh pengguna\n",
        "        sil_score = silhouette_score(X_clustered, labels_clustered, metric=metric)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error saat menghitung Silhouette Score: {e}\")\n",
        "        sil_score = np.nan\n",
        "\n",
        "    print(f\"üìä Menghitung Davies-Bouldin Index (Euclidean)...\")\n",
        "    try:\n",
        "        # Davies-Bouldin Index tidak memiliki parameter 'metric' (hanya Euclidean)\n",
        "        dbi_score = davies_bouldin_score(X_clustered, labels_clustered)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error saat menghitung Davies-Bouldin Index: {e}\")\n",
        "        dbi_score = np.nan\n",
        "    \n",
        "    return {\n",
        "        \"N_URLS\": len(labels),\n",
        "        \"N_CLUSTERS_TOTAL\": n_clusters_total,\n",
        "        \"N_CLUSTERS_ACTUAL\": n_clusters_actual,\n",
        "        \"N_OUTLIERS\": int(n_outliers),\n",
        "        \"SILHOUETTE_SCORE\": sil_score,\n",
        "        \"DAVIES_BOULDIN_INDEX\": dbi_score,\n",
        "        \"DISTANCE_METRIC\": metric,\n",
        "    }\n",
        "\n",
        "# =========================\n",
        "# Pipeline utama\n",
        "# =========================\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser(\n",
        "        description=\"Cluster NGINX URLs with BERT + OPTICS (RAM-friendly).\"\n",
        "    )\n",
        "    # ... (Semua Argumen) ...\n",
        "    parser.add_argument(\n",
        "        \"in_file\",\n",
        "        help=\"Input CSV hasil decoder (outputs/access_decoded.csv). Gunakan --decoded-csv.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"out_file\",\n",
        "        nargs=\"?\" if \"--evaluate\" in sys.argv else None, \n",
        "        help=\"Output CSV berisi label cluster per URL (mis. outputs/access_optics.csv). Diabaikan jika --evaluate.\",\n",
        "    )\n",
        "    parser.add_argument(\"-m\", type=int, default=5, help=\"OPTICS min_samples\")\n",
        "    parser.add_argument(\"-e\", type=float, default=0.5, help=\"OPTICS eps\")\n",
        "    parser.add_argument(\n",
        "        \"--topk\",\n",
        "        type=int,\n",
        "        default=200_000,\n",
        "        help=\"Ambil Top-K URL paling sering (semakin kecil semakin hemat RAM).\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--batch-size\", type=int, default=128, help=\"Batch size embedding BERT\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--max-length\",\n",
        "        type=int,\n",
        "        default=64,\n",
        "        help=\"Maks panjang token untuk BERT (64 cukup untuk mayoritas URL).\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--memmap\",\n",
        "        default=\"outputs/embeddings.dat\",\n",
        "        help=\"Path file memmap untuk embeddings (akan dibuat).\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--decoded-csv\",\n",
        "        action=\"store_true\",\n",
        "        help=\"Wajib diaktifkan: menandakan input adalah CSV hasil decoder.\",\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--evaluate\",\n",
        "        action=\"store_true\",\n",
        "        help=\"Aktifkan mode evaluasi metriks klastering (Silhouette dan DBI). Mengabaikan out_file untuk clustering.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--metric\",\n",
        "        type=str,\n",
        "        default=\"euclidean\",\n",
        "        choices=[\"euclidean\", \"cosine\"],\n",
        "        help=\"Metrik jarak yang digunakan untuk OPTICS dan evaluasi Silhouette Score.\",\n",
        "    )\n",
        "    \n",
        "    parser.add_argument(\n",
        "        \"-v\",\n",
        "        \"--verbose\",\n",
        "        action=\"store_true\",\n",
        "        help=\"Tampilkan rincian progress embedding, termasuk ETA dan rate.\",\n",
        "    )\n",
        "\n",
        "    # N_jobs untuk Scikit-learn (diabaikan oleh CuML)\n",
        "    parser.add_argument( \n",
        "        \"-j\",\n",
        "        \"--n-jobs\",\n",
        "        type=int,\n",
        "        default=-1 if not HAS_GPU_CLUSTERING else 1, \n",
        "        help=\"Jumlah core CPU yang digunakan untuk OPTICS (-1 untuk semua core). Diabaikan oleh CuML.\",\n",
        "    ) \n",
        "\n",
        "\n",
        "    args = parser.parse_args()\n",
        "    \n",
        "    # --- VALIDASI ARGUMEN ---\n",
        "    if not args.evaluate and not args.out_file:\n",
        "        parser.error(\"argumen 'out_file' wajib diberikan jika --evaluate tidak aktif.\")\n",
        "        \n",
        "    if not os.path.exists(args.in_file):\n",
        "        print(f\"‚ùå File tidak ditemukan: '{args.in_file}'\")\n",
        "        sys.exit(1)\n",
        "    if args.out_file:\n",
        "        out_dir = os.path.dirname(args.out_file)\n",
        "        if out_dir and not os.path.exists(out_dir):\n",
        "            os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    if not args.decoded_csv:\n",
        "        print(\n",
        "            \"‚ùå Harap gunakan --decoded-csv dan berikan input CSV dari app.decoder.\"\n",
        "        )\n",
        "        sys.exit(1)\n",
        "        \n",
        "    # LOGIKA BARU: Print mode evaluasi dan metrik yang digunakan\n",
        "    if args.evaluate:\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"         üåü MODE EVALUASI METRIK AKTIF üåü\")\n",
        "        print(f\"   Metrik Jarak yang digunakan: {args.metric.upper()}\")\n",
        "        print(\"   (Digunakan untuk OPTICS Clustering dan Silhouette Score)\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "\n",
        "    # 1) Top-K URL (streaming)\n",
        "    print(\"üîé Menghitung Top-K URL dari decoded CSV (streaming)...\")\n",
        "    unique_urls = topk_urls(args.in_file, k=args.topk)\n",
        "    print(f\"‚úÖ Siap embed: {len(unique_urls):,} URL\")\n",
        "\n",
        "    if len(unique_urls) == 0:\n",
        "        print(\"‚ùå Tidak ada URL yang ditemukan. Periksa input CSV.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    # 2) Tokenisasi ringan string URL \n",
        "    print(\"üî§ Menyusun token string URL (ringan)...\")\n",
        "    tokenized_urls = [\" \".join(split_url_tokens(u)) for u in unique_urls]\n",
        "\n",
        "    # 3) Embedding BERT ‚Üí memmap (hemat RAM)\n",
        "    print(\"üß† Menghitung embedding BERT (stream ke memmap)...\")\n",
        "    memmap_path = generate_url_embeddings_to_memmap(\n",
        "        tokenized_urls,\n",
        "        args.memmap,\n",
        "        batch_size=args.batch_size,\n",
        "        max_length=args.max_length,\n",
        "        verbose=args.verbose, \n",
        "    )\n",
        "\n",
        "    # 4) Load memmap readonly, normalisasi, kemudian OPTICS\n",
        "    print(\"‚ú® Normalisasi vektor...\")\n",
        "    hidden_size = MODEL.config.hidden_size\n",
        "    embs = np.memmap(\n",
        "        memmap_path,\n",
        "        dtype=\"float32\",\n",
        "        mode=\"r\",\n",
        "        shape=(len(tokenized_urls), hidden_size),\n",
        "    )\n",
        "    \n",
        "    embs_normalized = normalize(embs) # NumPy Array (CPU)\n",
        "    del embs \n",
        "\n",
        "    # --- PENGELOLAAN DATA DAN KLATERING UNTUK GPU/CPU ---\n",
        "    \n",
        "    embs_device = embs_normalized # Default: menggunakan NumPy di CPU\n",
        "    if HAS_GPU_CLUSTERING:\n",
        "        print(\"üöÄ Transfer data ke GPU (CuPy)...\")\n",
        "        # Konversi NumPy (CPU) ke CuPy (GPU).\n",
        "        embs_device = cp.asarray(embs_normalized)\n",
        "        # embs_normalized tetap di CPU untuk evaluasi/penyimpanan\n",
        "    \n",
        "    print(f\"üß© OPTICS clustering (min_samples={args.m}, eps={args.e}, metric='{args.metric}')...\")\n",
        "    \n",
        "    clustering_start_time = time.time()\n",
        "    \n",
        "    # Parameter OPTICS\n",
        "    optics_params = {\n",
        "        'min_samples': args.m,\n",
        "        'eps': args.e,\n",
        "        'metric': args.metric, # Menggunakan metrik yang dipilih\n",
        "    }\n",
        "    \n",
        "    if not HAS_GPU_CLUSTERING: # Tambahkan parameter spesifik scikit-learn jika CPU\n",
        "        optics_params['n_jobs'] = args.n_jobs\n",
        "        # optics_params['algorithm'] = 'ball_tree' # Memaksa Ball Tree untuk Cosine/Euclidean dengan n_jobs\n",
        "    \n",
        "    # Lakukan Klastering (hanya perlu fit_predict satu kali)\n",
        "    optics = OPTICS_IMPL(**optics_params)\n",
        "    labels_device = optics.fit_predict(embs_device)\n",
        "    \n",
        "    clustering_elapsed = time.time() - clustering_start_time\n",
        "\n",
        "    # --- KONVERSI HASIL KEMBALI KE CPU ---\n",
        "    if HAS_GPU_CLUSTERING:\n",
        "        # Pindahkan label hasil CuPy (GPU) ke NumPy (CPU)\n",
        "        labels = cp.asnumpy(labels_device)\n",
        "        del embs_device # Bebaskan memori GPU\n",
        "    else:\n",
        "        labels = labels_device # Jika CPU, labels sudah berupa NumPy array\n",
        "        \n",
        "    print(f\"‚úÖ OPTICS selesai dalam {clustering_elapsed:.2f} detik.\")\n",
        "    # --- AKHIR PENGELOLAAN DATA ---\n",
        "\n",
        "\n",
        "    # 5) Evaluasi Metrik (Jika diminta)\n",
        "    if args.evaluate:\n",
        "        \n",
        "        # Panggil evaluate_clustering_metrics dengan args.metric yang dipilih\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\" ¬† ¬† ¬† ¬† üìä EVALUASI METRIK üìâ\")\n",
        "        print(\"=\"*50)\n",
        "        \n",
        "        metrics = evaluate_clustering_metrics(embs_normalized, labels, args.metric)\n",
        "        \n",
        "        # ... (Cetak hasil metrics) ...\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\" ¬† ¬† ¬† ¬† ¬†üìà HASIL METRIK KLATERING üìâ\")\n",
        "        print(\"=\"*50)\n",
        "        print(f\"Total URL diproses: {metrics['N_URLS']:,}\")\n",
        "        print(f\"Waktu Klastering OPTICS: {clustering_elapsed:.2f} detik\")\n",
        "        print(f\"Metrik Jarak OPTICS/Silhouette: {metrics['DISTANCE_METRIC']}\") \n",
        "        print(f\"OPTICS min_samples: {args.m}, eps: {args.e}\")\n",
        "        print(\"-\" * 50)\n",
        "        print(f\"Jumlah Klaster Nyata (selain -1): {metrics['N_CLUSTERS_ACTUAL']}\")\n",
        "        print(f\"Jumlah Outlier (Label -1): {metrics['N_OUTLIERS']:,}\")\n",
        "        print(\"-\" * 50)\n",
        "        \n",
        "        if not np.isnan(metrics['SILHOUETTE_SCORE']):\n",
        "            print(f\"Silhouette Score (Ideal: mendekati 1.0): {metrics['SILHOUETTE_SCORE']:.4f}\")\n",
        "        else:\n",
        "            print(f\"Silhouette Score: {metrics['SILHOUETTE_SCORE']}\")\n",
        "            \n",
        "        if not np.isnan(metrics['DAVIES_BOULDIN_INDEX']):\n",
        "            print(f\"Davies-Bouldin Index (Ideal: mendekati 0.0, Euclidean): {metrics['DAVIES_BOULDIN_INDEX']:.4f}\") \n",
        "        else:\n",
        "            print(f\"Davies-Bouldin Index: {metrics['DAVIES_BOULDIN_INDEX']} (Euclidean)\")\n",
        "            \n",
        "        print(\"=\"*50)\n",
        "        \n",
        "        del embs_normalized\n",
        "        sys.exit(0) \n",
        "\n",
        "    # 6) Simpan hasil \n",
        "    del embs_normalized \n",
        "    \n",
        "    print(\"üíæ Menyimpan hasil label...\")\n",
        "    df_label = pd.DataFrame({\"masked\": unique_urls, \"cluster\": labels}).sort_values(\n",
        "        by=\"cluster\"\n",
        "    )\n",
        "    df_label.to_csv(args.out_file, index=False, encoding=\"utf-8\")\n",
        "    print(f\"‚úÖ Clustering results saved to: {args.out_file}\")\n",
        "\n",
        "    # 7) Simpan juga daftar cluster ‚Üí .txt (opsional)\n",
        "    txt_path = f\"{args.out_file}.txt\"\n",
        "    unique_labels = sorted(set(labels), key=lambda x: (x == -1, x))\n",
        "    groups = {lab: [] for lab in unique_labels}\n",
        "    for i, lab in enumerate(labels):\n",
        "        groups[lab].append(unique_urls[i])\n",
        "    with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for lab in unique_labels:\n",
        "            title = \"Noise (-1)\" if lab == -1 else f\"Cluster {lab}\"\n",
        "            f.write(f\"\\n{title} ({len(groups[lab])} items):\\n\")\n",
        "            for u in groups[lab]:\n",
        "                f.write(f\" ¬†{u}\\n\")\n",
        "    print(f\"üìù Cluster listing disimpan: {txt_path}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31eTBTTDXA4m",
        "outputId": "3a01a8ab-8f3c-4cd9-cbb3-5770b3e5cda0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2AOdE0-kY4rc",
        "outputId": "7418f968-7075-4221-cb14-ad2698fa27f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "archive_2.zip  web_logs\n"
          ]
        }
      ],
      "source": [
        "!ls /content/drive/MyDrive/datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ZDH6cbOydVAP"
      },
      "outputs": [],
      "source": [
        "# !unzip /content/drive/MyDrive/datasets/archive_2.zip -d /content/drive/MyDrive/datasets/web_logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "PD0kzq5OZBDU"
      },
      "outputs": [],
      "source": [
        "!cp -r /content/drive/MyDrive/datasets/web_logs/access.log /content/inputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5kWUtszjB3t",
        "outputId": "f15d9e18-9c1f-4505-ba1c-616e1e34325d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "access.log  sample.log\n"
          ]
        }
      ],
      "source": [
        "!ls /content/inputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVMX34wQHAdr",
        "outputId": "7f371efe-3d82-4562-87fc-724710714145"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10365152\n"
          ]
        }
      ],
      "source": [
        "!python -c \"print(sum(1 for _ in open('inputs/access.log', encoding='utf-8', errors='ignore')))\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "9283833\n"
          ]
        }
      ],
      "source": [
        "!python -c \"print(sum(1 for _ in open('outputs/access_decoded.csv', encoding='utf-8', errors='ignore')))\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚è≥ Starting streaming decode to CSV...\n",
            "‚úÖ Streaming decode complete.\n",
            "‚úÖ Log file successfully converted to outputs/sample_decoded.csv\n"
          ]
        }
      ],
      "source": [
        "!.\\.venv\\Scripts\\python -m app.decoder inputs/sample.log outputs/sample_decoded.csv --csv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vuWKnEcgMZh-",
        "outputId": "37f7a5ca-df61-4026-aae8-3649e7582b16"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "UsageError: Cell magic `%%powershell` not found.\n"
          ]
        }
      ],
      "source": [
        "!.\\.venv\\Scripts\\python -u -m app.main outputs/access_decoded.csv outputs/access_optics2.csv -m 10 --topk 200000 --batch-size 128 --max-length 64 --decoded-csv --metric euclidean -v"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRZhMKXjbOHc",
        "outputId": "10166fd3-2b85-434c-ba14-45efbaed59b5"
      },
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
        "\n",
        "# # Baca hasil clustering\n",
        "# df = pd.read_csv(\"outputs/access_optics.csv\")\n",
        "# labels = df[\"cluster\"].values\n",
        "\n",
        "# # Baca embeddings yang sudah disimpan\n",
        "# X = np.load(\"outputs/embeddings.npy\")\n",
        "\n",
        "# # Jumlah cluster & outlier\n",
        "# n_outliers = np.sum(labels == -1)\n",
        "# n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "\n",
        "# print(f\"Jumlah cluster terbentuk: {n_clusters}\")\n",
        "# print(f\"Jumlah outlier terdeteksi: {n_outliers}\")\n",
        "\n",
        "# # Evaluasi kualitas cluster\n",
        "# mask = labels != -1\n",
        "# if np.sum(mask) > 1 and n_clusters > 1:\n",
        "#     sil_score = silhouette_score(X[mask], labels[mask])\n",
        "#     dbi_score = davies_bouldin_score(X[mask], labels[mask])\n",
        "#     print(f\"Silhouette Score: {sil_score:.4f}\")\n",
        "#     print(f\"Davies-Bouldin Index: {dbi_score:.4f}\")\n",
        "# else:\n",
        "#     print(\"‚ö†Ô∏è Tidak cukup cluster untuk menghitung Silhouette/DBI\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Om_cCWIlX7sA",
        "outputId": "f2b3671d-44c8-4c81-aeb2-cf4bbabbc913"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing app/report.py\n"
          ]
        }
      ],
      "source": [
        "# %%writefile app/report.py\n",
        "# # app/report.py\n",
        "# import os\n",
        "# import argparse\n",
        "# import pandas as pd\n",
        "# from collections import defaultdict\n",
        "# from html import escape\n",
        "\n",
        "# def generate_html_report(csv_path: str, html_path: str, title: str = \"OPTICS URL Clusters\",\n",
        "#                          show_noise=True, max_items_per_cluster=None):\n",
        "#     \"\"\"\n",
        "#     Membuat laporan HTML dari file CSV hasil clustering.\n",
        "#     CSV diharapkan punya kolom: 'masked' dan 'cluster'.\n",
        "#     \"\"\"\n",
        "#     if not os.path.exists(csv_path):\n",
        "#         raise FileNotFoundError(f\"CSV not found: {csv_path}\")\n",
        "\n",
        "#     df = pd.read_csv(csv_path)\n",
        "#     if not {\"masked\", \"cluster\"}.issubset(df.columns):\n",
        "#         raise ValueError(\"CSV must contain columns: 'masked' and 'cluster'\")\n",
        "\n",
        "#     # pastikan tipe cluster numerik (agar -1 terdeteksi sebagai noise)\n",
        "#     try:\n",
        "#         df[\"cluster\"] = pd.to_numeric(df[\"cluster\"])\n",
        "#     except Exception:\n",
        "#         pass\n",
        "\n",
        "#     total_items = len(df)\n",
        "#     noise_mask = df[\"cluster\"].eq(-1) if \"cluster\" in df else pd.Series([False]*len(df))\n",
        "#     noise_count = int(noise_mask.sum())\n",
        "#     if not show_noise:\n",
        "#         df = df[~noise_mask]\n",
        "\n",
        "#     # kelompokkan\n",
        "#     grouped = defaultdict(list)\n",
        "#     for _, row in df.iterrows():\n",
        "#         grouped[row[\"cluster\"]].append(str(row[\"masked\"]))\n",
        "\n",
        "#     # urutkan cluster by size desc (kecuali -1 selalu taruh paling akhir)\n",
        "#     def sort_key(k):\n",
        "#         if k == -1:\n",
        "#             return (1, 10**12)  # noise di akhir\n",
        "#         return (0, -len(grouped[k]))\n",
        "\n",
        "#     clusters_sorted = sorted(grouped.keys(), key=sort_key)\n",
        "#     num_clusters_non_noise = sum(1 for cid in clusters_sorted if cid != -1)\n",
        "\n",
        "#     # HTML template (inline CSS + sedikit JS)\n",
        "#     css = \"\"\"\n",
        "#     body { font-family: system-ui, -apple-system, Segoe UI, Roboto, Arial, sans-serif; margin: 24px; }\n",
        "#     h1 { margin-bottom: 0.2rem; }\n",
        "#     .muted { color: #666; }\n",
        "#     .stats { display: flex; gap: 16px; flex-wrap: wrap; margin: 12px 0 20px; }\n",
        "#     .stat { background: #f4f6f8; border: 1px solid #e5e7eb; border-radius: 12px; padding: 12px 16px; }\n",
        "#     .search { margin: 16px 0 22px; }\n",
        "#     input[type=\"search\"] { width: 100%; padding: 10px 12px; border: 1px solid #d1d5db; border-radius: 10px; }\n",
        "#     details { border: 1px solid #e5e7eb; border-radius: 12px; margin: 10px 0; overflow: hidden; background: #fff; }\n",
        "#     summary { cursor: pointer; padding: 12px 16px; background: #f9fafb; font-weight: 600; display: flex; justify-content: space-between; align-items: center; }\n",
        "#     .count { font-size: 12px; background: #eef2ff; color: #3730a3; border-radius: 999px; padding: 4px 8px; }\n",
        "#     ul { margin: 0; padding: 10px 22px 16px 32px; }\n",
        "#     li { line-height: 1.5; word-break: break-all; }\n",
        "#     .noise { background: #fff7ed; }\n",
        "#     .hidden { display: none !important; }\n",
        "#     footer { margin-top: 28px; font-size: 12px; color: #888; }\n",
        "#     \"\"\"\n",
        "\n",
        "#     js = \"\"\"\n",
        "#     function filterClusters() {\n",
        "#       const q = document.getElementById('q').value.toLowerCase();\n",
        "#       const blocks = document.querySelectorAll('.cluster');\n",
        "#       blocks.forEach(block => {\n",
        "#         const items = Array.from(block.querySelectorAll('li'));\n",
        "#         let any = false;\n",
        "#         items.forEach(li => {\n",
        "#           const show = li.textContent.toLowerCase().includes(q);\n",
        "#           li.classList.toggle('hidden', !show);\n",
        "#           if (show) any = true;\n",
        "#         });\n",
        "#         block.classList.toggle('hidden', !any);\n",
        "#       });\n",
        "#       // update summary counts after filter\n",
        "#       document.querySelectorAll('.cluster').forEach(block => {\n",
        "#         const total = block.querySelectorAll('li:not(.hidden)').length;\n",
        "#         const badge = block.querySelector('.count');\n",
        "#         if (badge) badge.textContent = total + \" item\";\n",
        "#       });\n",
        "#     }\n",
        "#     \"\"\"\n",
        "\n",
        "#     # build body\n",
        "#     parts = []\n",
        "#     parts.append(f\"<h1>{escape(title)}</h1>\")\n",
        "#     parts.append(f'<div class=\"muted\">Sumber: {escape(os.path.basename(csv_path))}</div>')\n",
        "#     parts.append(\n",
        "#         '<div class=\"stats\">'\n",
        "#         f'<div class=\"stat\"><b>Total item</b><br>{total_items:,}</div>'\n",
        "#         f'<div class=\"stat\"><b>Total cluster</b><br>{num_clusters_non_noise:,}</div>'\n",
        "#         f'<div class=\"stat\"><b>Noise (-1)</b><br>{noise_count:,}</div>'\n",
        "#         '</div>'\n",
        "#     )\n",
        "#     parts.append(\n",
        "#         '<div class=\"search\">'\n",
        "#         '<input id=\"q\" type=\"search\" placeholder=\"Cari URL di semua cluster‚Ä¶\" oninput=\"filterClusters()\">'\n",
        "#         '</div>'\n",
        "#     )\n",
        "\n",
        "#     for cid in clusters_sorted:\n",
        "#         items = grouped[cid]\n",
        "#         if max_items_per_cluster is not None:\n",
        "#             shown = items[:max_items_per_cluster]\n",
        "#             truncated = len(items) - len(shown)\n",
        "#         else:\n",
        "#             shown = items\n",
        "#             truncated = 0\n",
        "\n",
        "#         is_noise = (cid == -1)\n",
        "#         cls = \"cluster noise\" if is_noise else \"cluster\"\n",
        "#         label = \"Noise (-1)\" if is_noise else f\"Cluster {cid}\"\n",
        "#         count_str = f\"{len(shown):,} item\" if truncated == 0 else f\"{len(shown):,} item (+\"+f\"{truncated:,} tersembunyi)\"\n",
        "#         parts.append(f'<details class=\"{cls}\"><summary>{escape(label)} <span class=\"count\">{count_str}</span></summary>')\n",
        "#         parts.append(\"<ul>\")\n",
        "#         for url in shown:\n",
        "#             parts.append(f\"<li>{escape(url)}</li>\")\n",
        "#         if truncated > 0:\n",
        "#             parts.append(f'<li class=\"muted\">‚Ä¶ dan {truncated:,} item lainnya tidak ditampilkan</li>')\n",
        "#         parts.append(\"</ul></details>\")\n",
        "\n",
        "#     parts.append('<footer>Generated by report.py</footer>')\n",
        "\n",
        "#     html = f\"\"\"<!doctype html>\n",
        "# <html lang=\"id\">\n",
        "# <head>\n",
        "# <meta charset=\"utf-8\">\n",
        "# <meta name=\"viewport\" content=\"width=device-width,initial-scale=1\">\n",
        "# <title>{escape(title)}</title>\n",
        "# <style>{css}</style>\n",
        "# </head>\n",
        "# <body>\n",
        "# {''.join(parts)}\n",
        "# <script>{js}</script>\n",
        "# </body>\n",
        "# </html>\"\"\"\n",
        "\n",
        "#     os.makedirs(os.path.dirname(html_path) or \".\", exist_ok=True)\n",
        "#     with open(html_path, \"w\", encoding=\"utf-8\") as f:\n",
        "#         f.write(html)\n",
        "#     return html_path\n",
        "\n",
        "# def main():\n",
        "#     p = argparse.ArgumentParser(description=\"Generate HTML report from clustering CSV\")\n",
        "#     p.add_argument(\"csv\", help=\"Path ke outputs/access_optics.csv\")\n",
        "#     p.add_argument(\"html\", help=\"Path output .html (mis. outputs/cluster_report.html)\")\n",
        "#     p.add_argument(\"--title\", default=\"OPTICS URL Clusters\", help=\"Judul laporan\")\n",
        "#     p.add_argument(\"--hide-noise\", action=\"store_true\", help=\"Sembunyikan cluster -1 (noise)\")\n",
        "#     p.add_argument(\"--max-per-cluster\", type=int, default=None, help=\"Batasi jumlah item ditampilkan per cluster\")\n",
        "#     args = p.parse_args()\n",
        "\n",
        "#     path = generate_html_report(\n",
        "#         csv_path=args.csv,\n",
        "#         html_path=args.html,\n",
        "#         title=args.title,\n",
        "#         show_noise=not args.hide_noise,\n",
        "#         max_items_per_cluster=args.max_per_cluster\n",
        "#     )\n",
        "#     print(f\"‚úÖ Report tersimpan: {path}\")\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Report tersimpan: outputs/cluster_report.html\n"
          ]
        }
      ],
      "source": [
        "!.\\.venv\\Scripts\\python -m app.report outputs/access_optics.csv outputs/cluster_report.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting app/expand_originals.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app/expand_originals.py\n",
        "import os\n",
        "import re\n",
        "import csv\n",
        "import argparse\n",
        "import sqlite3\n",
        "from collections import defaultdict, Counter\n",
        "from html import escape\n",
        "import math\n",
        "\n",
        "MASK_RE = re.compile(r\"\\d+\")\n",
        "\n",
        "def mask_url(url: str) -> str:\n",
        "    return MASK_RE.sub(\"<NUM>\", url or \"\")\n",
        "\n",
        "def load_clustered_masked(masked_cluster_csv):\n",
        "    masked_to_cluster = {}\n",
        "    cluster_to_masked = defaultdict(set)\n",
        "    with open(masked_cluster_csv, \"r\", encoding=\"utf-8\", newline=\"\") as f:\n",
        "        r = csv.DictReader(f)\n",
        "        if \"masked\" not in r.fieldnames or \"cluster\" not in r.fieldnames:\n",
        "            raise ValueError(\"CSV cluster harus punya kolom 'masked' dan 'cluster'\")\n",
        "        for row in r:\n",
        "            m = row[\"masked\"]\n",
        "            c = int(row[\"cluster\"])\n",
        "            masked_to_cluster[m] = c\n",
        "            cluster_to_masked[c].add(m)\n",
        "    return masked_to_cluster, cluster_to_masked\n",
        "\n",
        "# -------- Aggregator per original URL ‚Üí per-IP buckets --------\n",
        "class OrigAgg:\n",
        "    __slots__ = (\"total\", \"ip_cnt\", \"ip_sample\")\n",
        "    def __init__(self):\n",
        "        self.total = 0\n",
        "        self.ip_cnt = Counter()\n",
        "        self.ip_sample = {}  # ip -> first-seen row dict\n",
        "\n",
        "def format_log_line(row: dict) -> str:\n",
        "    def g(k, default=\"\"):\n",
        "        v = row.get(k, default) if row else default\n",
        "        return \"\" if v is None else str(v)\n",
        "    ip = g(\"ip\", \"-\")\n",
        "    time = g(\"time\", \"-\")\n",
        "    method = g(\"method\", \"-\")\n",
        "    url = g(\"url\", \"-\")\n",
        "    protocol = g(\"protocol\", \"-\")\n",
        "    status = g(\"status\", \"-\")\n",
        "    size = g(\"size\", \"-\")\n",
        "    ref = g(\"referrer\", \"-\")\n",
        "    ua = g(\"user_agent\", \"-\")\n",
        "    extra = g(\"extra\", \"-\")\n",
        "    return f'{ip} - - [{time}] \"{method} {url} {protocol}\" {status} {size} \"{ref}\" \"{ua}\" \"{extra}\"'\n",
        "\n",
        "def stream_count_originals(decoded_csv, masked_to_cluster,\n",
        "                            max_originals_per_masked=200,\n",
        "                            top_ips_per_original=3):\n",
        "    masked_counts = Counter()\n",
        "    store: dict[str, dict[str, OrigAgg]] = defaultdict(dict)\n",
        "\n",
        "    with open(decoded_csv, \"r\", encoding=\"utf-8\", errors=\"ignore\", newline=\"\") as f:\n",
        "        r = csv.DictReader(f)\n",
        "        need = {\"ip\",\"time\",\"method\",\"url\",\"protocol\",\"status\",\"size\",\"referrer\",\"user_agent\",\"extra\"}\n",
        "        if not need.issubset(set(r.fieldnames or [])):\n",
        "            raise ValueError(\"CSV decoded harus punya kolom: \" + \", \".join(sorted(need)))\n",
        "\n",
        "        for row in r:\n",
        "            orig = row[\"url\"]\n",
        "            m = mask_url(orig)\n",
        "            if m not in masked_to_cluster:\n",
        "                continue\n",
        "\n",
        "            masked_counts[m] += 1\n",
        "            bucket = store[m]\n",
        "\n",
        "            if orig not in bucket and len(bucket) >= max_originals_per_masked:\n",
        "                top = sorted(bucket.items(), key=lambda kv: kv[1].total, reverse=True)[:max_originals_per_masked]\n",
        "                bucket.clear()\n",
        "                bucket.update(top)\n",
        "\n",
        "            agg = bucket.get(orig)\n",
        "            if agg is None:\n",
        "                agg = OrigAgg()\n",
        "                bucket[orig] = agg\n",
        "\n",
        "            agg.total += 1\n",
        "            ip = row.get(\"ip\") or \"-\"\n",
        "            agg.ip_cnt[ip] += 1\n",
        "            if ip not in agg.ip_sample:  # first-seen per IP\n",
        "                agg.ip_sample[ip] = {\n",
        "                    \"ip\": row.get(\"ip\"),\n",
        "                    \"time\": row.get(\"time\"),\n",
        "                    \"method\": row.get(\"method\"),\n",
        "                    \"url\": row.get(\"url\"),\n",
        "                    \"protocol\": row.get(\"protocol\"),\n",
        "                    \"status\": row.get(\"status\"),\n",
        "                    \"size\": row.get(\"size\"),\n",
        "                    \"referrer\": row.get(\"referrer\"),\n",
        "                    \"user_agent\": row.get(\"user_agent\"),\n",
        "                    \"extra\": row.get(\"extra\"),\n",
        "                }\n",
        "\n",
        "    # optional trim per original\n",
        "    if top_ips_per_original is not None:\n",
        "        for _, bucket in store.items():\n",
        "            for _, agg in bucket.items():\n",
        "                keep_ips = set(ip for ip, _ in agg.ip_cnt.most_common(top_ips_per_original*4))\n",
        "                agg.ip_cnt = Counter({ip: cnt for ip, cnt in agg.ip_cnt.items() if ip in keep_ips})\n",
        "                agg.ip_sample = {ip: agg.ip_sample[ip] for ip in list(agg.ip_sample.keys()) if ip in keep_ips}\n",
        "\n",
        "    return masked_counts, store\n",
        "\n",
        "# ---------- SQLite ----------\n",
        "def init_db(db_path):\n",
        "    os.makedirs(os.path.dirname(db_path) or \".\", exist_ok=True)\n",
        "    conn = sqlite3.connect(db_path)\n",
        "    cur = conn.cursor()\n",
        "    cur.execute(\"PRAGMA journal_mode=WAL;\")\n",
        "    cur.execute(\"PRAGMA synchronous=NORMAL;\")\n",
        "    cur.executescript(\"\"\"\n",
        "    DROP TABLE IF EXISTS clusters;\n",
        "    DROP TABLE IF EXISTS masked;\n",
        "    DROP TABLE IF EXISTS originals;\n",
        "    DROP TABLE IF EXISTS original_ips;\n",
        "\n",
        "    CREATE TABLE clusters(\n",
        "      cluster_id INTEGER PRIMARY KEY\n",
        "    );\n",
        "\n",
        "    CREATE TABLE masked(\n",
        "      masked TEXT PRIMARY KEY,\n",
        "      cluster_id INTEGER,\n",
        "      masked_total INTEGER,\n",
        "      FOREIGN KEY(cluster_id) REFERENCES clusters(cluster_id)\n",
        "    );\n",
        "\n",
        "    CREATE TABLE originals(\n",
        "      masked TEXT,\n",
        "      original_url TEXT,\n",
        "      total_cnt INTEGER,\n",
        "      PRIMARY KEY(masked, original_url),\n",
        "      FOREIGN KEY(masked) REFERENCES masked(masked)\n",
        "    );\n",
        "\n",
        "    CREATE TABLE original_ips(\n",
        "      masked TEXT,\n",
        "      original_url TEXT,\n",
        "      ip TEXT,\n",
        "      cnt INTEGER,\n",
        "      sample_line TEXT,\n",
        "      PRIMARY KEY(masked, original_url, ip),\n",
        "      FOREIGN KEY(masked, original_url) REFERENCES originals(masked, original_url)\n",
        "    );\n",
        "\n",
        "    CREATE INDEX idx_masked_cluster ON masked(cluster_id);\n",
        "    CREATE INDEX idx_originals_masked ON originals(masked);\n",
        "    CREATE INDEX idx_origips_masked ON original_ips(masked);\n",
        "    \"\"\")\n",
        "    conn.commit()\n",
        "    return conn\n",
        "\n",
        "def save_to_sqlite(db_path, masked_to_cluster, masked_counts, store,\n",
        "                   top_per_masked=20, top_ips_per_original=3):\n",
        "    conn = init_db(db_path)\n",
        "    cur = conn.cursor()\n",
        "\n",
        "    clusters = sorted(set(masked_to_cluster.values()), key=lambda x:(x==-1, x))\n",
        "    cur.executemany(\"INSERT INTO clusters(cluster_id) VALUES(?)\", [(c,) for c in clusters])\n",
        "\n",
        "    cur.executemany(\n",
        "        \"INSERT INTO masked(masked, cluster_id, masked_total) VALUES(?,?,?)\",\n",
        "        [(m, masked_to_cluster[m], int(masked_counts.get(m, 0))) for m in masked_to_cluster]\n",
        "    )\n",
        "\n",
        "    batch_orig, batch_ip = [], []\n",
        "    for m, bucket in store.items():\n",
        "        for orig, agg in sorted(bucket.items(), key=lambda kv: kv[1].total, reverse=True)[:top_per_masked]:\n",
        "            batch_orig.append((m, orig, int(agg.total)))\n",
        "            for ip, cnt in agg.ip_cnt.most_common(top_ips_per_original):\n",
        "                sample = format_log_line(agg.ip_sample.get(ip))\n",
        "                batch_ip.append((m, orig, ip, int(cnt), sample))\n",
        "            if len(batch_orig) >= 50_000:\n",
        "                cur.executemany(\"INSERT OR REPLACE INTO originals(masked, original_url, total_cnt) VALUES(?,?,?)\", batch_orig)\n",
        "                batch_orig.clear()\n",
        "            if len(batch_ip) >= 50_000:\n",
        "                cur.executemany(\"INSERT OR REPLACE INTO original_ips(masked, original_url, ip, cnt, sample_line) VALUES(?,?,?,?,?)\", batch_ip)\n",
        "                batch_ip.clear()\n",
        "\n",
        "    if batch_orig:\n",
        "        cur.executemany(\"INSERT OR REPLACE INTO originals(masked, original_url, total_cnt) VALUES(?,?,?)\", batch_orig)\n",
        "    if batch_ip:\n",
        "        cur.executemany(\"INSERT OR REPLACE INTO original_ips(masked, original_url, ip, cnt, sample_line) VALUES(?,?,?,?,?)\", batch_ip)\n",
        "\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "def _q(conn, sql, args=()):\n",
        "    cur = conn.execute(sql, args)\n",
        "    cols = [c[0] for c in cur.description]\n",
        "    for row in cur:\n",
        "        yield dict(zip(cols, row))\n",
        "\n",
        "BASE_CSS = \"\"\"\n",
        "body{font-family:Arial, sans-serif; margin:24px;}\n",
        "details{border:1px solid #ddd; border-radius:10px; padding:10px 12px; margin:12px 0; background:#fff;}\n",
        "summary{cursor:pointer; font-weight:bold; outline:none}\n",
        ".badge{display:inline-block; padding:2px 8px; border-radius:12px; background:#eef; margin-left:8px; color:#334}\n",
        "table{border-collapse:collapse; width:100%; margin:8px 0 14px}\n",
        "th,td{border:1px solid #eee; padding:6px; text-align:left}\n",
        "th{background:#fafafa}\n",
        ".small{color:#666; font-size:12px}\n",
        "pre{margin:0; font-family:ui-monospace, SFMono-Regular, Menlo, Consolas, \"Liberation Mono\", monospace; font-size:12px; white-space:pre-wrap}\n",
        "\"\"\"\n",
        "\n",
        "def write_text(path, text):\n",
        "    os.makedirs(os.path.dirname(path) or \".\", exist_ok=True)\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(text)\n",
        "\n",
        "# ---------- Single-page COLLAPSIBLE (tanpa search & sorting) ----------\n",
        "def generate_collapsible_single_html(\n",
        "    db_path,\n",
        "    out_html,\n",
        "    top_per_masked=20,\n",
        "    max_masked_per_cluster=200,\n",
        "    top_ips_per_original=3,\n",
        "):\n",
        "    \"\"\"\n",
        "    Single-page HTML:\n",
        "      - Cluster ‚Üí <details>\n",
        "      - Di dalam cluster: render hingga N masked (MUNCUL SEKALI per masked)\n",
        "      - Untuk tiap masked:\n",
        "          * tampilkan ringkasan (masked, total hits)\n",
        "          * tabel Original URL (top-K)\n",
        "          * untuk setiap original URL: tabel IP (top-M IP) dengan sample-line per IP\n",
        "    \"\"\"\n",
        "    conn = sqlite3.connect(db_path)\n",
        "    clusters = list(_q(conn, \"\"\"\n",
        "        SELECT cluster_id, COUNT(*) AS n_masked, SUM(masked_total) AS hits\n",
        "        FROM masked GROUP BY cluster_id ORDER BY (cluster_id=-1), cluster_id\n",
        "    \"\"\"))\n",
        "\n",
        "    blocks = []\n",
        "    total_masked_rendered = 0\n",
        "\n",
        "    for c in clusters:\n",
        "        cid = c[\"cluster_id\"]\n",
        "        title = \"Noise (-1)\" if cid == -1 else f\"Cluster {cid}\"\n",
        "        n_masked = c[\"n_masked\"]\n",
        "        hits = c[\"hits\"] or 0\n",
        "\n",
        "        # ambil masked yang terbesar di cluster ini\n",
        "        masked_rows = list(_q(conn, \"\"\"\n",
        "            SELECT masked, masked_total\n",
        "            FROM masked\n",
        "            WHERE cluster_id=?\n",
        "            ORDER BY masked_total DESC, masked\n",
        "            LIMIT ?\n",
        "        \"\"\", (cid, max_masked_per_cluster)))\n",
        "\n",
        "        masked_blocks = []\n",
        "        for row in masked_rows:\n",
        "            m = row[\"masked\"]\n",
        "            masked_total = row[\"masked_total\"] or 0\n",
        "\n",
        "            # top-N original URL untuk masked ini\n",
        "            originals = list(_q(conn, \"\"\"\n",
        "                SELECT original_url, total_cnt\n",
        "                FROM originals\n",
        "                WHERE masked=?\n",
        "                ORDER BY total_cnt DESC\n",
        "                LIMIT ?\n",
        "            \"\"\", (m, top_per_masked)))\n",
        "\n",
        "            # bangun tabel original URL\n",
        "            orig_rows_html = []\n",
        "            for orig in originals:\n",
        "                o_url = orig[\"original_url\"]\n",
        "                o_total = orig[\"total_cnt\"] or 0\n",
        "\n",
        "                # top-M IP untuk original ini\n",
        "                ip_rows = list(_q(conn, \"\"\"\n",
        "                    SELECT ip, cnt, sample_line\n",
        "                    FROM original_ips\n",
        "                    WHERE masked=? AND original_url=?\n",
        "                    ORDER BY cnt DESC\n",
        "                    LIMIT ?\n",
        "                \"\"\", (m, o_url, top_ips_per_original)))\n",
        "\n",
        "                # tabel IP (di-bungkus agar rapi)\n",
        "                ip_tbl_rows = []\n",
        "                for ipr in ip_rows:\n",
        "                    share = (ipr[\"cnt\"] / masked_total) if masked_total else 0.0\n",
        "                    ip_tbl_rows.append(\n",
        "                        \"<tr>\"\n",
        "                        f\"<td>{escape(ipr['ip'] or '-')}</td>\"\n",
        "                        f\"<td><pre>{escape(ipr['sample_line'] or '')}</pre></td>\"\n",
        "                        f\"<td>{ipr['cnt']}</td>\"\n",
        "                        f\"<td>{share:.2%}</td>\"\n",
        "                        \"</tr>\"\n",
        "                    )\n",
        "\n",
        "                ip_table_html = (\n",
        "                    \"<table>\"\n",
        "                    \"<thead><tr><th>IP</th><th>Sample Log Line (first seen per IP)</th><th>Count</th><th>Share</th></tr></thead>\"\n",
        "                    f\"<tbody>{''.join(ip_tbl_rows)}</tbody>\"\n",
        "                    \"</table>\"\n",
        "                )\n",
        "\n",
        "                # baris untuk original URL (URL dan totalnya), lalu tabel IP di bawahnya\n",
        "                orig_rows_html.append(\n",
        "                    \"<tr>\"\n",
        "                    f\"<td style='width:40%;word-break:break-all'>{escape(o_url)}</td>\"\n",
        "                    f\"<td style='width:10%'>{o_total}</td>\"\n",
        "                    f\"<td>{ip_table_html}</td>\"\n",
        "                    \"</tr>\"\n",
        "                )\n",
        "\n",
        "            if not orig_rows_html:\n",
        "                # kalau tidak ada original (jarang terjadi), skip blok masked\n",
        "                continue\n",
        "\n",
        "            # satu blok <details> PER MASKED (muncul sekali)\n",
        "            masked_blocks.append(f\"\"\"\n",
        "            <details>\n",
        "              <summary><code>{escape(m)}</code>\n",
        "                <span class=\"badge\">masked hits: {masked_total}</span>\n",
        "                <span class=\"badge\">showing top {top_per_masked} originals √ó top {top_ips_per_original} IP</span>\n",
        "              </summary>\n",
        "              <table>\n",
        "                <thead>\n",
        "                  <tr><th>Original URL</th><th>Total Hits (URL)</th><th>Per-IP Samples</th></tr>\n",
        "                </thead>\n",
        "                <tbody>\n",
        "                  {''.join(orig_rows_html)}\n",
        "                </tbody>\n",
        "              </table>\n",
        "            </details>\n",
        "            \"\"\")\n",
        "\n",
        "        cluster_html = f\"\"\"\n",
        "        <div class=\"cluster-container\">\n",
        "          <details>\n",
        "            <summary>{escape(title)}\n",
        "              <span class=\"badge\">masked: {n_masked}</span>\n",
        "              <span class=\"badge\">hits: {hits}</span>\n",
        "              <span class=\"small\">render up to {min(max_masked_per_cluster, n_masked)} masked</span>\n",
        "            </summary>\n",
        "            {''.join(masked_blocks)}\n",
        "          </details>\n",
        "        </div>\n",
        "        \"\"\"\n",
        "        blocks.append(cluster_html)\n",
        "        total_masked_rendered += len(masked_rows)\n",
        "\n",
        "    conn.close()\n",
        "\n",
        "    html = f\"\"\"<!doctype html>\n",
        "<html><head><meta charset=\"utf-8\">\n",
        "<title>Cluster Report (Cluster ‚Üí Masked ‚Üí Original ‚Üí IP)</title>\n",
        "<style>\n",
        "{BASE_CSS}\n",
        "/* kecilkan pre agar muat */\n",
        "pre{{margin:0; font-size:12px; white-space:pre-wrap}}\n",
        "code{{background:#f6f8fa; padding:2px 6px; border-radius:6px}}\n",
        "summary .badge{{margin-left:6px}}\n",
        "table td{{vertical-align:top}}\n",
        "</style>\n",
        "</head><body>\n",
        "<h2>Cluster Report ‚Äî Grouped by Masked (no search/sort)</h2>\n",
        "<div class=\"small\">Rendered masked total (max per cluster): {total_masked_rendered}</div>\n",
        "{\"\".join(blocks)}\n",
        "</body></html>\"\"\"\n",
        "    write_text(out_html, html)\n",
        "\n",
        "\n",
        "# ---------- (opsional) Multi-page sederhana (tanpa search/sort) ----------\n",
        "def generate_site_from_sqlite(db_path, outdir, page_size=50, top_per_masked=20, top_ips_per_original=3):\n",
        "    conn = sqlite3.connect(db_path)\n",
        "\n",
        "    clusters = list(_q(conn, \"SELECT cluster_id, COUNT(*) AS n_masked, SUM(masked_total) AS hits FROM masked GROUP BY cluster_id ORDER BY (cluster_id=-1), cluster_id\"))\n",
        "    rows = \"\".join(\n",
        "        f\"<tr><td><a href='cluster_{c['cluster_id']}_1.html'>{'Noise (-1)' if c['cluster_id']==-1 else 'Cluster '+str(c['cluster_id'])}</a></td>\"\n",
        "        f\"<td>{c['n_masked']}</td><td>{c['hits'] or 0}</td></tr>\"\n",
        "        for c in clusters\n",
        "    )\n",
        "    idx_html = f\"\"\"<!doctype html><html><head><meta charset='utf-8'><title>Clusters Index</title>\n",
        "<style>{BASE_CSS}</style></head><body>\n",
        "<h2>Clusters Index</h2>\n",
        "<table><thead><tr><th>Cluster</th><th># Masked</th><th>Total Hits</th></tr></thead><tbody>\n",
        "{rows}\n",
        "</tbody></table>\n",
        "</body></html>\"\"\"\n",
        "    write_text(os.path.join(outdir, \"index.html\"), idx_html)\n",
        "\n",
        "    cluster_ids = [c[\"cluster_id\"] for c in clusters]\n",
        "    for cid in cluster_ids:\n",
        "        total = next(_q(conn, \"SELECT COUNT(*) AS n FROM masked WHERE cluster_id=?\", (cid,)))[\"n\"]\n",
        "        pages = max(1, math.ceil(total / page_size))\n",
        "        for page in range(1, pages+1):\n",
        "            offset = (page-1)*page_size\n",
        "            masked_rows = list(_q(conn,\n",
        "                \"SELECT masked, masked_total FROM masked WHERE cluster_id=? ORDER BY masked_total DESC, masked LIMIT ? OFFSET ?\",\n",
        "                (cid, page_size, offset)\n",
        "            ))\n",
        "            title = \"Noise (-1)\" if cid == -1 else f\"Cluster {cid}\"\n",
        "\n",
        "            blocks = []\n",
        "            for row in masked_rows:\n",
        "                m = row[\"masked\"]\n",
        "                masked_total = row[\"masked_total\"] or 0\n",
        "                originals = list(_q(conn,\n",
        "                    \"SELECT original_url, total_cnt FROM originals WHERE masked=? ORDER BY total_cnt DESC LIMIT ?\",\n",
        "                    (m, top_per_masked)\n",
        "                ))\n",
        "                orows = \"\"\n",
        "                for orig in originals:\n",
        "                    ip_rows = list(_q(conn,\n",
        "                        \"SELECT ip, cnt, sample_line FROM original_ips WHERE masked=? AND original_url=? ORDER BY cnt DESC LIMIT ?\",\n",
        "                        (m, orig[\"original_url\"], top_ips_per_original)\n",
        "                    ))\n",
        "                    for ipr in ip_rows:\n",
        "                        share = (ipr[\"cnt\"] / masked_total) if masked_total else 0.0\n",
        "                        orows += f\"<tr><td>{escape(ipr['ip'] or '-')}</td><td><pre>{escape(ipr['sample_line'] or '')}</pre></td><td>{ipr['cnt']}</td><td>{share:.2%}</td></tr>\"\n",
        "\n",
        "                blocks.append(f\"\"\"\n",
        "                <div class='cluster-container'>\n",
        "                  <div><strong>{escape(m)}</strong>\n",
        "                    <span class='badge'>hits: {masked_total}</span>\n",
        "                  </div>\n",
        "                  <table><thead><tr><th>IP</th><th>Sample Log Line</th><th>Count</th><th>Share</th></tr></thead>\n",
        "                  <tbody>{orows}</tbody></table>\n",
        "                </div>\n",
        "                \"\"\")\n",
        "\n",
        "            pager = \" \".join(\n",
        "                (f\"<strong>{p}</strong>\" if p==page else f\"<a href='cluster_{cid}_{p}.html'>{p}</a>\")\n",
        "                for p in range(1, pages+1)\n",
        "            )\n",
        "            html = f\"\"\"<!doctype html><html><head><meta charset='utf-8'><title>{title}</title>\n",
        "<style>{BASE_CSS}</style></head><body>\n",
        "<p><a href='index.html'>&larr; Back to index</a></p>\n",
        "<h2>{title}</h2>\n",
        "<div class='pager'>Pages: {pager}</div>\n",
        "{''.join(blocks)}\n",
        "<div class='pager'>Pages: {pager}</div>\n",
        "</body></html>\"\"\"\n",
        "            os.makedirs(outdir, exist_ok=True)\n",
        "            write_text(os.path.join(outdir, f\"cluster_{cid}_{page}.html\"), html)\n",
        "\n",
        "    conn.close()\n",
        "\n",
        "def main():\n",
        "    ap = argparse.ArgumentParser(description=\"Expand masked clusters ‚Üí per-original per-IP sample lines (SQLite, no search/sort).\")\n",
        "    ap.add_argument(\"decoded_csv\", help=\"outputs/access_decoded.csv (hasil decoder)\")\n",
        "    ap.add_argument(\"cluster_csv\", help=\"outputs/access_optics.csv (masked,cluster)\")\n",
        "    ap.add_argument(\"--top-per-masked\", type=int, default=20, help=\"Top-N original URL per masked\")\n",
        "    ap.add_argument(\"--top-ips-per-original\", type=int, default=3, help=\"Top-M IP per original URL\")\n",
        "    ap.add_argument(\"--max-originals-per-masked\", type=int, default=200, help=\"Batas variasi original per masked\")\n",
        "\n",
        "    # SQLite + HTML\n",
        "    ap.add_argument(\"--db\", default=\"outputs/cluster_report.db\", help=\"SQLite DB output\")\n",
        "    ap.add_argument(\"--single-html\", default=\"outputs/expanded_collapsible.html\", help=\"Single-page HTML (collapsible, no search/sort)\")\n",
        "    ap.add_argument(\"--max-masked-per-cluster\", type=int, default=200, help=\"Batas masked dirender per cluster\")\n",
        "\n",
        "    # (opsional) Multi-page\n",
        "    ap.add_argument(\"--site-dir\", default=\"\", help=\"Folder output HTML statis (multi-page, optional)\")\n",
        "    ap.add_argument(\"--page-size\", type=int, default=50, help=\"# masked per halaman (multi-page optional)\")\n",
        "    args = ap.parse_args()\n",
        "\n",
        "    print(\"üîπ Load clustered masked ‚Üí cluster...\")\n",
        "    masked_to_cluster, _ = load_clustered_masked(args.cluster_csv)\n",
        "    print(f\"  masked patterns: {len(masked_to_cluster):,}\")\n",
        "\n",
        "    print(\"üîπ Streaming decoded CSV ‚Üí agregasi per original & per-IP...\")\n",
        "    masked_counts, store = stream_count_originals(\n",
        "        args.decoded_csv,\n",
        "        masked_to_cluster,\n",
        "        max_originals_per_masked=args.max_originals_per_masked,\n",
        "        top_ips_per_original=args.top_ips_per_original,\n",
        "    )\n",
        "\n",
        "    print(f\"üîπ Simpan ke SQLite: {args.db}\")\n",
        "    save_to_sqlite(args.db, masked_to_cluster, masked_counts, store,\n",
        "                   top_per_masked=args.top_per_masked,\n",
        "                   top_ips_per_original=args.top_ips_per_original)\n",
        "\n",
        "    if args.single_html:\n",
        "        print(f\"üîπ Generate single HTML (collapsible) ‚Üí {args.single_html}\")\n",
        "        generate_collapsible_single_html(args.db, args.single_html,\n",
        "                                         top_per_masked=args.top_per_masked,\n",
        "                                         max_masked_per_cluster=args.max_masked_per_cluster,\n",
        "                                         top_ips_per_original=args.top_ips_per_original)\n",
        "\n",
        "    if args.site_dir:\n",
        "        print(f\"üîπ Generate HTML statis (multi-page) ‚Üí {args.site_dir}\")\n",
        "        generate_site_from_sqlite(args.db, args.site_dir,\n",
        "                                  page_size=args.page_size,\n",
        "                                  top_per_masked=args.top_per_masked,\n",
        "                                  top_ips_per_original=args.top_ips_per_original)\n",
        "\n",
        "    print(\"‚úÖ Done.\")\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîπ Load clustered masked ‚Üí cluster...\n",
            "  masked patterns: 92,150\n",
            "üîπ Streaming decoded CSV ‚Üí agregasi per original & per-IP...\n",
            "üîπ Simpan ke SQLite: outputs\\cluster_report.db\n",
            "üîπ Generate single HTML (collapsible) ‚Üí outputs\\expanded_collapsible.html\n",
            "‚úÖ Done.\n"
          ]
        }
      ],
      "source": [
        "! .\\.venv\\Scripts\\python -m app.expand_originals outputs\\access_decoded.csv outputs\\access_optics.csv --db outputs\\cluster_report.db --single-html outputs\\expanded_collapsible.html --top-per-masked 20 --top-ips-per-original 3 --max-originals-per-masked 200 --max-masked-per-cluster 200"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
